

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Implementing Distributed Training &#8212; project-x 0.0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/sphinx_highlight.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'latest/model-dev-guide/dtrain/dtrain-implement';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Configuration Templates" href="config-templates.html" />
    <link rel="prev" title="Distributed Training Concepts" href="dtrain-introduction.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
    <p class="title logo__title">project-x 0.0.1 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Welcome to project-x’s documentation!
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../architecture/_index.html">How Determined Works</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../architecture/introduction.html">Introduction to Determined</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../architecture/system-architecture.html">System Architecture</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../example-solutions/_index.html">Examples</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../integrations/_index.html">Integrations</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../integrations/ecosystem/_index.html">Ecosystem Integration</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../integrations/notification/_index.html">Monitoring Experiment Through Webhooks</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../integrations/notification/zapier.html">Through Zapier</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../integrations/notification/slack.html">Through Slack</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../integrations/prometheus/_index.html">Configure Determined with Prometheus and Grafana</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../interfaces/_index.html">Tools</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../interfaces/cli-ug.html">Determined CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../interfaces/commands-and-shells.html">Commands and Shells</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../interfaces/ide-integration.html">IDE Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../interfaces/notebooks.html">Jupyter Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../interfaces/proxy-ports.html">Exposing Custom Ports</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../interfaces/tensorboard.html">Using TensorBoard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../interfaces/webui-if.html">Web Interface (WebUI)</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../_index.html">Model Developer Guide</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../apis-howto/_index.html">Training APIs</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../apis-howto/api-core-ug.html">Core API User Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../apis-howto/api-pytorch-ug.html">PyTorch API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../apis-howto/api-keras-ug.html">Keras API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../apis-howto/api-estimator-ug.html">Estimator API</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../batch-processing/_index.html">Torch Batch Processing API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../best-practices/_index.html">Best Practices</a></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="_index.html">Distributed Training with Determined</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="dtrain-introduction.html">Distributed Training Concepts</a></li>
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Implementing Distributed Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="config-templates.html">Configuration Templates</a></li>
<li class="toctree-l3"><a class="reference internal" href="reproducibility.html">Reproducibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="optimize-training.html">Optimizing Training</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../model-hub-library/_index.html">Model Hub Library</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../model-hub-library/mmdetection/_index.html">MMDetection</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../model-hub-library/transformers/_index.html">Transformers</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../model-hub-library/transformers/tutorial.html">Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../model-hub-library/transformers/examples.html">Examples</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../reference/_index.html">Reference</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../reference/cli-reference.html">Determined CLI Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../reference/python-sdk.html">Python SDK</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../reference/rest-api.html">REST API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../reference/batch-processing/_index.html">Batch Processing API Reference</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../reference/deploy/_index.html">Deployment Reference</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../reference/deploy/config/_index.html">Config Reference</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../reference/deploy/config/agent-config-reference.html">Agent Configuration Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../reference/deploy/config/common-config-options.html">Common Configuration Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../reference/deploy/config/helm-config-reference.html">Helm Chart Configuration Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../reference/deploy/config/master-config-reference.html">Master Configuration Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../reference/interface/_index.html">Job Configuration Reference</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../reference/model-hub/_index.html">Model Hub APIs</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../reference/model-hub/mmdetection-api.html">MMDetection API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/model-hub/transformers-api.html">Transformers API</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../reference/searcher/_index.html">Custom Searcher Reference</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../reference/training/_index.html">Training Reference</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../reference/training/api-core-reference.html"><code class="docutils literal notranslate"><span class="pre">det.core</span></code> API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/training/api-deepspeed-reference.html"><code class="docutils literal notranslate"><span class="pre">det.pytorch.deepspeed</span></code> API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/training/api-det-reference.html"><code class="docutils literal notranslate"><span class="pre">det</span></code> API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/training/api-estimator-reference.html"><code class="docutils literal notranslate"><span class="pre">det.estimator</span></code> API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/training/api-keras-reference.html"><code class="docutils literal notranslate"><span class="pre">det.keras</span></code> API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/training/api-pytorch-reference.html"><code class="docutils literal notranslate"><span class="pre">det.pytorch</span></code> API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/training/api-pytorch-samplers-reference.html"><code class="docutils literal notranslate"><span class="pre">det.pytorch.samplers</span></code> API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/training/experiment-config-reference.html">Experiment Configuration Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../release-notes/_index.html">Release Notes</a></li>

<li class="toctree-l1 has-children"><a class="reference internal" href="../../setup-cluster/_index.html">Setup</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/_index.html">Set Up Determined</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/aws/_index.html">Deploy on AWS</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/aws/aws-spot.html">Use Spot Instances</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/aws/dynamic-agents-aws.html">Deploy Determined with Dynamic Agents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/aws/install-on-aws.html">Install Determined</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/gcp/_index.html">Deploy on GCP</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/gcp/dynamic-agents-gcp.html">Deploy Determined with Dynamic Agents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/gcp/install-gcp.html">Install Determined</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/_index.html">Deploy on Kubernetes</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/custom-pod-specs.html">Customize a Pod</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/helm-commands.html">Helm and Kubectl Command Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/install-on-kubernetes.html">Install Determined on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/k8s-dev-guide.html">Development Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/setup-aks-cluster.html">Set up and Manage an Azure Kubernetes Service (AKS) Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/setup-eks-cluster.html">Set up and Manage an AWS Kubernetes (EKS) Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/setup-gke-cluster.html">Set up and Manage a Google Kubernetes Engine (GKE) Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/troubleshooting.html">Troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/_index.html">Deploy on Prem</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/deploy.html">Install Determined Using <code class="docutils literal notranslate"><span class="pre">det</span> <span class="pre">deploy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/docker.html">Install Determined Using Docker</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/homebrew.html">Install Determined Using Homebrew (macOS)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/linux-packages.html">Install Determined Using Linux Packages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/requirements.html">Installation Requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/wsl.html">Install Determined Using Windows Subsystem for Linux (Windows)</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/_index.html">Deploy on Slurm/PBS</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/hpc-launching-architecture.html">HPC Launching Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/hpc-security-considerations.html">HPC Launcher Security Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/install-on-slurm.html">Install Determined on Slurm/PBS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/singularity.html">Provide a Container Image Cache</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/slurm-known-issues.html">Known Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/slurm-requirements.html">Installation Requirements</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../setup-cluster/security/_index.html">Security</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/oauth.html">OAuth 2.0 Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/oidc.html">OpenID Connect Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/rbac.html">RBAC</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/saml.html">SAML Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/scim.html">SCIM Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/tls.html">Transport Layer Security</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/_index.html">Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/pytorch-mnist-local-qs.html">Run Your First Experiment in Determined</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/pytorch-mnist-tutorial.html">PyTorch MNIST Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/pytorch-porting-tutorial.html">PyTorch Porting Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/quickstart-mdldev.html">Quickstart for Model Developers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/tf-mnist-tutorial.html">TensorFlow Keras Fashion MNIST Tutorial</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../attributions.html">Open Source Licenses</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/latest/model-dev-guide/dtrain/dtrain-implement.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Implementing Distributed Training</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connectivity">Connectivity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#configuration">Configuration</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#slots-per-trial">Slots Per Trial</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#global-batch-size">Global Batch Size</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-optimizations">Advanced Optimizations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#downloading-data">Downloading Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scheduling-behavior">Scheduling Behavior</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-inference">Distributed Inference</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="implementing-distributed-training">
<span id="multi-gpu-training-implement"></span><h1>Implementing Distributed Training<a class="headerlink" href="#implementing-distributed-training" title="Permalink to this heading">#</a></h1>
<section id="connectivity">
<h2>Connectivity<a class="headerlink" href="#connectivity" title="Permalink to this heading">#</a></h2>
<p>Multi-machine training necessitates that all machines are capable of establishing a direct
connection. Firewall rules or network configurations might exist that prevent machines in your
cluster from communicating with each other. You can verify that agent machines can connect with each
other outside of Determined by using tools such as <code class="docutils literal notranslate"><span class="pre">ping</span></code> or <code class="docutils literal notranslate"><span class="pre">netcat</span></code>.</p>
<p>More rarely, if agents have multiple network interfaces and some of them are not routable,
Determined may pick one of those interfaces rather than one that allows one agent to contact
another. In this case, it is possible to explicitly set the network interface used for distributed
training, as described in <a class="reference internal" href="../../setup-cluster/_index.html#cluster-configuration"><span class="std std-ref">Basic Setup: Step 7 - Configure the Cluster</span></a>.</p>
</section>
<section id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Permalink to this heading">#</a></h2>
<section id="slots-per-trial">
<h3>Slots Per Trial<a class="headerlink" href="#slots-per-trial" title="Permalink to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">resources.slots_per_trial</span></code> field in the <a class="reference internal" href="../../reference/training/experiment-config-reference.html#experiment-config-reference"><span class="std std-ref">experiment configuration</span></a> controls the number of GPUs used to train a single trial.</p>
<p>By default, this field is set to a value of <code class="docutils literal notranslate"><span class="pre">1</span></code>, which disables distributed training. If you
increase the <code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code> value, this will automatically enable multi-GPU training. Bear in
mind that these GPUs can either be located on a single machine or distributed across multiple
machines. The experiment configuration merely dictates the number of GPUs to be used in the training
process, while the Determined job scheduler decides whether to schedule the task on a single agent
or multiple agents. Whether the job scheduler schedules the task on a single agent or multiple
agents depends on the machines in the cluster and other active workloads.</p>
<p>Multi-machine parallelism allows you to further parallelize training across more GPUs. To use this
feature, set <code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code> to a multiple of the total number of GPUs on an agent machine. For
example, if your resource pool consists of multiple 8-GPU agent machines, valid <code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code>
values would be 16, 24, 32, and so on.</p>
<p>In the following configuration, trials will use the combined resources of multiple machines to train
a model:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">resources</span><span class="p">:</span>
<span class="w">  </span><span class="nt">slots_per_trial</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16</span><span class="w">  </span><span class="c1"># Two 8-GPU agent machines will be used in a trial</span>
</pre></div>
</div>
<p>For distributed multi-machine training, Determined will automatically detect a common network
interface that is shared by the agent machines. If your cluster has multiple common network
interfaces, we advise specifying the fastest one in <a class="reference internal" href="../../setup-cluster/_index.html#cluster-configuration"><span class="std std-ref">Step 7 - Configure the Cluster</span></a> under
<code class="docutils literal notranslate"><span class="pre">task_container_defaults.dtrain_network_interface</span></code>.</p>
<p>When the <code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code> field is set, the per-slot (i.e., per-GPU) batch size is set to
<code class="docutils literal notranslate"><span class="pre">global_batch_size</span> <span class="pre">//</span> <span class="pre">slots_per_trial</span></code>. The per-slot and global batch sizes can be accessed
through the context using <code class="xref py py-func docutils literal notranslate"><span class="pre">context.get_per_slot_batch_size()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">context.get_global_batch_size()</span></code>, respectively. If <code class="docutils literal notranslate"><span class="pre">global_batch_size</span></code> is not
evenly divisible by <code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code>, the remainder is dropped.</p>
<p>When scheduling a multi-machine distributed training job, Determined prefers that the job use all of
the slots (GPUs) on an agent. The section on <a class="reference internal" href="#dtrain-scheduling"><span class="std std-ref">Scheduling Behavior</span></a>
describes this preference in more detail.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You might have existing tasks that are running on a single machine that are preventing your
multi-GPU trials from acquiring sufficient GPUs. To alleviate this, you may want to consider
adjusting <code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code> or terminating existing tasks to free up slots in your cluster.</p>
</div>
</section>
<section id="global-batch-size">
<h3>Global Batch Size<a class="headerlink" href="#global-batch-size" title="Permalink to this heading">#</a></h3>
<p>You can reduce computational overhead by setting the <code class="docutils literal notranslate"><span class="pre">global_batch_size</span></code> to the largest batch size
that fits into a single GPU multiplied times the number of slots.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">global_batch_size</span></code> field will be automatically respected by the Trial APIs. To use this
hyperparameter with the Core API, you’ll need to reference <code class="docutils literal notranslate"><span class="pre">global_batch_size</span></code> explicitly and
organize your code to respect its value.</p>
</div>
<p>During distributed training, the <code class="docutils literal notranslate"><span class="pre">global_batch_size</span></code> specified in the <a class="reference internal" href="../../reference/training/experiment-config-reference.html#experiment-config-reference"><span class="std std-ref">experiment
configuration file</span></a> is partitioned across <code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code> GPUs.
The per-GPU batch size is set to: <code class="docutils literal notranslate"><span class="pre">global_batch_size</span> <span class="pre">//</span> <span class="pre">slots_per_trial</span></code>. Recall that if
<code class="docutils literal notranslate"><span class="pre">global_batch_size</span></code> is not evenly divisible by <code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code>, the remainder is dropped. For
convenience, the per-GPU batch size can be accessed via the Trial API, using
<code class="xref py py-func docutils literal notranslate"><span class="pre">context.get_per_slot_batch_size</span></code>.</p>
<p>For improved performance, <em>weak-scaling</em> is recommended. Weak-scaling means proportionally
increasing your <code class="docutils literal notranslate"><span class="pre">global_batch_size</span></code> with <code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code>. For example, you might change
<code class="docutils literal notranslate"><span class="pre">global_batch_size</span></code> and <code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code> from 32 and 1 to 128 and 4, respectively. You can
visit the blog post, <a class="reference external" href="https://developer.hpe.com/blog/scaling-deep-learning-workloads/">Scaling deep learning workloads</a>, to learn more about weak
scaling.</p>
<p>Note that adjusting <code class="docutils literal notranslate"><span class="pre">global_batch_size</span></code> can impact your model convergence, which in turn can
affect your training and/or testing accuracy. You might need to adjust model hyperparameters, such
as the learning rate, or consider using a different optimizer when training with larger batch sizes.</p>
</section>
<section id="advanced-optimizations">
<span id="multi-gpu-training-implement-adv-optimizations"></span><h3>Advanced Optimizations<a class="headerlink" href="#advanced-optimizations" title="Permalink to this heading">#</a></h3>
<p>The following optimizations can further reduce training time.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">optimizations.aggregation_frequency</span></code> controls how many batches are evaluated before exchanging
gradients. This optimization increases your effective batch size to <code class="docutils literal notranslate"><span class="pre">aggregation_frequency</span></code> *
<code class="docutils literal notranslate"><span class="pre">global_batch_size</span></code>. <code class="docutils literal notranslate"><span class="pre">optimizations.aggregation_frequency</span></code> is useful in scenarios where
directly increasing the batch size is not possible (for example, due to GPU memory limitations).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">optimizations.gradient_compression</span></code> reduces the time it takes to transfer gradients between
GPUs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">optimizations.auto_tune_tensor_fusion</span></code> automatically identifies the optimal message size
during gradient transfers, thereby reducing communication overhead.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">optimizations.average_training_metrics</span></code> averages the training metrics across GPUs at the end
of every training workload, a process that requires communication. <code class="docutils literal notranslate"><span class="pre">average_training_metrics</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">true</span></code> by default and typically does not have a significant impact on training
performance. However, if you have a very small <code class="docutils literal notranslate"><span class="pre">scheduling_unit</span></code>, disabling this option could
improve performance. When disabled, only the training metrics from the chief GPU are reported.
This impacts results shown in the WebUI and TensorBoard but does not influence model behavior or
hyperparameter search.</p></li>
</ul>
<p>To learn more about these optimizations, visit the <a class="reference internal" href="../../reference/training/experiment-config-reference.html#exp-config-optimizations"><span class="std std-ref">optimizations</span></a>
section in the Experiment Configuration Reference.</p>
<p>If you’re not seeing improved performance with distributed training, your model might have a
performance bottleneck that can’t be directly alleviated by using multiple GPUs, such as with data
loading. You’re encouraged to experiment with a synthetic dataset in order to verify the performance
of multi-GPU training.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Multi-machine distributed training is designed to maximize performance by training with all the
resources of a machine. This can lead to situations where an experiment is created but never
becomes active, such as when the number of GPUs requested does not factor into (divide evenly)
the machines available, or when another experiment is already using some GPUs on a machine.</p>
<p>If an experiment does not become active after a minute or so, please ensure that
<code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code> is a multiple of the number of GPUs available on a machine. You can also use
the CLI command <code class="docutils literal notranslate"><span class="pre">det</span> <span class="pre">task</span> <span class="pre">list</span></code> to check if any other tasks are using GPUs and preventing your
experiment from using all the GPUs on a machine.</p>
</div>
</section>
</section>
<section id="downloading-data">
<h2>Downloading Data<a class="headerlink" href="#downloading-data" title="Permalink to this heading">#</a></h2>
<p>When performing distributed training, Determined automatically creates one process for each GPU that
is being used for training. Each of these processes attempts to download training and/or validation
data, so it is important to ensure that concurrent data downloads do not conflict with one another.</p>
<p>One way to achieve this is to include a unique identifier in the local file system path where the
downloaded data is stored. A convenient identifier is the <code class="docutils literal notranslate"><span class="pre">rank</span></code> of the current process. The
process <code class="docutils literal notranslate"><span class="pre">rank</span></code> is automatically assigned by Determined and is unique among all trial processes.
You can accomplish this by leveraging the <code class="xref py py-func docutils literal notranslate"><span class="pre">self.context.distributed.get_rank()</span></code> function.</p>
<p>The following example demonstrates how to accomplish this when downloading data from S3. In this
example, the S3 bucket name is configured via a <code class="docutils literal notranslate"><span class="pre">data.bucket</span></code> field in the experiment
configuration file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">import</span> <span class="nn">os</span>


<span class="k">def</span> <span class="nf">download_data_from_s3</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">s3_bucket</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">get_data_config</span><span class="p">()[</span><span class="s2">&quot;bucket&quot;</span><span class="p">]</span>
    <span class="n">download_directory</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;/tmp/data-rank</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">data_file</span> <span class="o">=</span> <span class="s2">&quot;data.csv&quot;</span>

    <span class="n">s3</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s2">&quot;s3&quot;</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">download_directory</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">download_directory</span><span class="p">,</span> <span class="n">data_file</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">filepath</span><span class="p">):</span>
        <span class="n">s3</span><span class="o">.</span><span class="n">download_file</span><span class="p">(</span><span class="n">s3_bucket</span><span class="p">,</span> <span class="n">data_file</span><span class="p">,</span> <span class="n">filepath</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">download_directory</span>
</pre></div>
</div>
</section>
<section id="scheduling-behavior">
<span id="dtrain-scheduling"></span><h2>Scheduling Behavior<a class="headerlink" href="#scheduling-behavior" title="Permalink to this heading">#</a></h2>
<p>The Determined master schedules distributed training jobs automatically, ensuring that all of the
compute resources required for a job are available before the job is launched. Here are some
important details regarding <code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code> and the scheduler’s behavior:</p>
<ul class="simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code> is less than or equal to the number of slots on a single agent, Determined
considers scheduling multiple distributed training jobs on a single agent. This approach is
designed to improve utilization and to allow multiple small training jobs to run on a single
agent. For example, an agent with eight GPUs could be assigned two 4-GPU jobs or four 2-GPU jobs.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code> is greater than the number of slots on a single agent, Determined
schedules the distributed training job onto multiple agents. To ensure good performance and
utilize the full network bandwidth of each machine and to minimize inter-machine networking,
Determined prefers utilizing all of the agent GPUs on a machine. For example, if all the agents
in your cluster have eight GPUs each, you should submit jobs with <code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code> set to a
multiple of eight, such as 8, 16, or 24.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The scheduler can find fits for distributed jobs against agents of different sizes. This is
configured via the <a class="reference internal" href="../../reference/deploy/config/master-config-reference.html#allow-uneven-slots"><span class="std std-ref">allowing_heterogeneous_fits</span></a> parameter. This
parameter defaults to <code class="docutils literal notranslate"><span class="pre">false</span></code>. By default Determined requires that the job use all of the slots
(GPUs) on an agent.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If these scheduling constraints for multi-machine distributed training are not satisfied, and you
have not configured the <a class="reference internal" href="../../reference/deploy/config/master-config-reference.html#allow-uneven-slots"><span class="std std-ref">allowing_heterogeneous_fits</span></a> parameter,
distributed training jobs are not scheduled and wait indefinitely. For example, if every agent in
the cluster has eight GPUs, a job with <code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code> set to <code class="docutils literal notranslate"><span class="pre">12</span></code> is never scheduled.</p>
<p>If a multi-GPU experiment does not become active after a minute or so, please ensure that
<code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code> is set so that it can be scheduled within these constraints. You can also use
the CLI command <code class="docutils literal notranslate"><span class="pre">det</span> <span class="pre">task</span> <span class="pre">list</span></code> to check if any other tasks are using GPUs and preventing your
experiment from using all the GPUs on a machine.</p>
</div>
</section>
<section id="distributed-inference">
<h2>Distributed Inference<a class="headerlink" href="#distributed-inference" title="Permalink to this heading">#</a></h2>
<p>PyTorch users have the option to use the existing distributed training workflow with PyTorchTrial to
accelerate their inference workloads. This workflow is not yet officially supported, therefore,
users must specify certain training-specific artifacts that are not used for inference. To run a
distributed batch inference job, create a new PyTorchTrial and follow these steps:</p>
<ul class="simple">
<li><p>Load the trained model and build the inference dataset using <code class="docutils literal notranslate"><span class="pre">build_validation_data_loader()</span></code>.</p></li>
<li><p>Specify the inference step using <code class="docutils literal notranslate"><span class="pre">evaluate_batch()</span></code> or <code class="docutils literal notranslate"><span class="pre">evaluate_full_dataset()</span></code>.</p></li>
<li><p>Register a dummy <code class="docutils literal notranslate"><span class="pre">optimizer</span></code>.</p></li>
<li><p>Specify a <code class="docutils literal notranslate"><span class="pre">build_training_data_loader()</span></code> that returns a dummy dataloader.</p></li>
<li><p>Specify a no-op <code class="docutils literal notranslate"><span class="pre">train_batch()</span></code> that returns an empty map of metrics.</p></li>
</ul>
<p>Once the new PyTorchTrial object is created, use the experiment configuration to distribute
inference in the same way as training. <a class="reference external" href="https://github.com/determined-ai/determined/blob/master/examples/computer_vision/cifar10_pytorch_inference/">cifar10_pytorch_inference</a>
serves as an example of distributed batch inference.</p>
</section>
</section>


                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="dtrain-introduction.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Distributed Training Concepts</p>
      </div>
    </a>
    <a class="right-next"
       href="config-templates.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Configuration Templates</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connectivity">Connectivity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#configuration">Configuration</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#slots-per-trial">Slots Per Trial</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#global-batch-size">Global Batch Size</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-optimizations">Advanced Optimizations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#downloading-data">Downloading Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scheduling-behavior">Scheduling Behavior</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-inference">Distributed Inference</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Tara & LB
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023, Determined AI.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>