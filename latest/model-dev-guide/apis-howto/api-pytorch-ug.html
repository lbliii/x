

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<meta content="Learn how to train a PyTorch model in Determined. This user guide covers everything from PyTorch's tensor operations, data loading, and preprocessing techniques, to how to train and evaluate your models using Determined AI's PyTorch Trial and PyTorch Trainer." name="description" />

    <title>PyTorch API &#8212; project-x 0.0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/sphinx_highlight.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'latest/model-dev-guide/apis-howto/api-pytorch-ug';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Keras API" href="api-keras-ug.html" />
    <link rel="prev" title="Core API User Guide" href="api-core-ug.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
    <p class="title logo__title">project-x 0.0.1 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Welcome to project-x’s documentation!
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../architecture/_index.html">How Determined Works</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../architecture/introduction.html">Introduction to Determined</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../architecture/system-architecture.html">System Architecture</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../example-solutions/_index.html">Examples</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../integrations/_index.html">Integrations</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../integrations/ecosystem/_index.html">Ecosystem Integration</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../integrations/notification/_index.html">Monitoring Experiment Through Webhooks</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../integrations/notification/zapier.html">Through Zapier</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../integrations/notification/slack.html">Through Slack</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../integrations/prometheus/_index.html">Configure Determined with Prometheus and Grafana</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../interfaces/_index.html">Tools</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../interfaces/cli-ug.html">Determined CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../interfaces/commands-and-shells.html">Commands and Shells</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../interfaces/ide-integration.html">IDE Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../interfaces/notebooks.html">Jupyter Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../interfaces/proxy-ports.html">Exposing Custom Ports</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../interfaces/tensorboard.html">Using TensorBoard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../interfaces/webui-if.html">Web Interface (WebUI)</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../_index.html">Model Developer Guide</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active has-children"><a class="reference internal" href="_index.html">Training APIs</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="api-core-ug.html">Core API User Guide</a></li>
<li class="toctree-l3 current active"><a class="current reference internal" href="#">PyTorch API</a></li>
<li class="toctree-l3"><a class="reference internal" href="api-keras-ug.html">Keras API</a></li>
<li class="toctree-l3"><a class="reference internal" href="api-estimator-ug.html">Estimator API</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../batch-processing/_index.html">Torch Batch Processing API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../best-practices/_index.html">Best Practices</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../dtrain/_index.html">Distributed Training with Determined</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../dtrain/dtrain-introduction.html">Distributed Training Concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../dtrain/dtrain-implement.html">Implementing Distributed Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../dtrain/config-templates.html">Configuration Templates</a></li>
<li class="toctree-l3"><a class="reference internal" href="../dtrain/reproducibility.html">Reproducibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../dtrain/optimize-training.html">Optimizing Training</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../model-hub-library/_index.html">Model Hub Library</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../model-hub-library/mmdetection/_index.html">MMDetection</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../model-hub-library/transformers/_index.html">Transformers</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../model-hub-library/transformers/tutorial.html">Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../model-hub-library/transformers/examples.html">Examples</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../reference/_index.html">Reference</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../reference/cli-reference.html">Determined CLI Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../reference/python-sdk.html">Python SDK</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../reference/rest-api.html">REST API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../reference/batch-processing/_index.html">Batch Processing API Reference</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../reference/deploy/_index.html">Deployment Reference</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../reference/deploy/config/_index.html">Config Reference</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../reference/deploy/config/agent-config-reference.html">Agent Configuration Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../reference/deploy/config/common-config-options.html">Common Configuration Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../reference/deploy/config/helm-config-reference.html">Helm Chart Configuration Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../reference/deploy/config/master-config-reference.html">Master Configuration Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../reference/interface/_index.html">Job Configuration Reference</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../reference/model-hub/_index.html">Model Hub APIs</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../reference/model-hub/mmdetection-api.html">MMDetection API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/model-hub/transformers-api.html">Transformers API</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../reference/searcher/_index.html">Custom Searcher Reference</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../reference/training/_index.html">Training Reference</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../reference/training/api-core-reference.html"><code class="docutils literal notranslate"><span class="pre">det.core</span></code> API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/training/api-deepspeed-reference.html"><code class="docutils literal notranslate"><span class="pre">det.pytorch.deepspeed</span></code> API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/training/api-det-reference.html"><code class="docutils literal notranslate"><span class="pre">det</span></code> API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/training/api-estimator-reference.html"><code class="docutils literal notranslate"><span class="pre">det.estimator</span></code> API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/training/api-keras-reference.html"><code class="docutils literal notranslate"><span class="pre">det.keras</span></code> API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/training/api-pytorch-reference.html"><code class="docutils literal notranslate"><span class="pre">det.pytorch</span></code> API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/training/api-pytorch-samplers-reference.html"><code class="docutils literal notranslate"><span class="pre">det.pytorch.samplers</span></code> API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/training/experiment-config-reference.html">Experiment Configuration Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../release-notes/_index.html">Release Notes</a></li>

<li class="toctree-l1 has-children"><a class="reference internal" href="../../setup-cluster/_index.html">Setup</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/_index.html">Set Up Determined</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/aws/_index.html">Deploy on AWS</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/aws/aws-spot.html">Use Spot Instances</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/aws/dynamic-agents-aws.html">Deploy Determined with Dynamic Agents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/aws/install-on-aws.html">Install Determined</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/gcp/_index.html">Deploy on GCP</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/gcp/dynamic-agents-gcp.html">Deploy Determined with Dynamic Agents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/gcp/install-gcp.html">Install Determined</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/_index.html">Deploy on Kubernetes</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/custom-pod-specs.html">Customize a Pod</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/helm-commands.html">Helm and Kubectl Command Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/install-on-kubernetes.html">Install Determined on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/k8s-dev-guide.html">Development Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/setup-aks-cluster.html">Set up and Manage an Azure Kubernetes Service (AKS) Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/setup-eks-cluster.html">Set up and Manage an AWS Kubernetes (EKS) Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/setup-gke-cluster.html">Set up and Manage a Google Kubernetes Engine (GKE) Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/troubleshooting.html">Troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/_index.html">Deploy on Prem</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/deploy.html">Install Determined Using <code class="docutils literal notranslate"><span class="pre">det</span> <span class="pre">deploy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/docker.html">Install Determined Using Docker</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/homebrew.html">Install Determined Using Homebrew (macOS)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/linux-packages.html">Install Determined Using Linux Packages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/requirements.html">Installation Requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/wsl.html">Install Determined Using Windows Subsystem for Linux (Windows)</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/_index.html">Deploy on Slurm/PBS</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/hpc-launching-architecture.html">HPC Launching Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/hpc-security-considerations.html">HPC Launcher Security Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/install-on-slurm.html">Install Determined on Slurm/PBS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/singularity.html">Provide a Container Image Cache</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/slurm-known-issues.html">Known Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/slurm-requirements.html">Installation Requirements</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../setup-cluster/security/_index.html">Security</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/oauth.html">OAuth 2.0 Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/oidc.html">OpenID Connect Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/rbac.html">RBAC</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/saml.html">SAML Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/scim.html">SCIM Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/tls.html">Transport Layer Security</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/_index.html">Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/pytorch-mnist-local-qs.html">Run Your First Experiment in Determined</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/pytorch-mnist-tutorial.html">PyTorch MNIST Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/pytorch-porting-tutorial.html">PyTorch Porting Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/quickstart-mdldev.html">Quickstart for Model Developers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/tf-mnist-tutorial.html">TensorFlow Keras Fashion MNIST Tutorial</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../attributions.html">Open Source Licenses</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/latest/model-dev-guide/apis-howto/api-pytorch-ug.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>PyTorch API</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-trial">PyTorch Trial</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#download-data">Download Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-data">Load Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initializing-objects">Initializing Objects</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizers">Optimizers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-schedulers">Learning Rate Schedulers</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-training-loop">Define the Training Loop</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-step">Optimization Step</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpointing">Checkpointing</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-validation-loop">Define the Validation Loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#callbacks">Callbacks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-usage">Advanced Usage</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-clipping">Gradient Clipping</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reducing-metrics">Reducing Metrics</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#customize-a-reproducible-dataset">Customize a Reproducible Dataset</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#profiling">Profiling</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#porting-checklist">Porting Checklist</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#remove-pinned-gpus">Remove Pinned GPUs</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#remove-distributed-training-code">Remove Distributed Training Code</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#get-hyperparameters-from-pytorchtrialcontext">Get Hyperparameters from PyTorchTrialContext</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-trainer">PyTorch Trainer</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initializing-the-trainer">Initializing the Trainer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#run-your-training-script-locally">Run Your Training Script Locally</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#local-distributed-training">Local Distributed Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-mode">Test Mode</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-your-training-code-for-deploying-to-a-determined-cluster">Prepare Your Training Code for Deploying to a Determined Cluster</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#submit-your-trial-for-training-on-cluster">Submit Your Trial for Training on Cluster</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-checkpoints">Loading Checkpoints</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="pytorch-api">
<h1>PyTorch API<a class="headerlink" href="#pytorch-api" title="Permalink to this heading">#</a></h1>
<p>In this guide, you’ll learn how to use <a class="reference internal" href="#pytorch-trial-ug"><span class="std std-ref">PyTorch Trial</span></a> and <a class="reference internal" href="#pytorch-trainer-ug"><span class="std std-ref">PyTorch Trainer</span></a>.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Visit the API reference</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="../../reference/training/api-pytorch-reference.html#pytorch-api-ref"><span class="std std-ref">determined.pytorch.PyTorchTrial</span></a></p></td>
</tr>
</tbody>
</table>
<section id="pytorch-trial">
<span id="pytorch-trial-ug"></span><h2>PyTorch Trial<a class="headerlink" href="#pytorch-trial" title="Permalink to this heading">#</a></h2>
<p>This section guides you through training a PyTorch model in Determined. You need to implement a
trial class that inherits from <code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrial</span></code> and specify it as the
entrypoint in the <a class="reference internal" href="../../reference/training/experiment-config-reference.html#experiment-config-reference"><span class="std std-ref">experiment configuration</span></a>.</p>
<p>To implement a <code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrial</span></code>, you need to override specific functions
that represent the components that are used in the training procedure. It is helpful to work off of
a skeleton to keep track of what is still required. A good starting template can be found below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Sequence</span>
<span class="kn">from</span> <span class="nn">determined.pytorch</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">PyTorchTrial</span><span class="p">,</span> <span class="n">PyTorchTrialContext</span>

<span class="n">TorchData</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>

<span class="k">class</span> <span class="nc">MyTrial</span><span class="p">(</span><span class="n">PyTorchTrial</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">PyTorchTrialContext</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">context</span>

    <span class="k">def</span> <span class="nf">build_training_data_loader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLoader</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">build_validation_data_loader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLoader</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">train_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TorchData</span><span class="p">,</span> <span class="n">epoch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span>  <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">evaluate_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TorchData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{}</span>
</pre></div>
</div>
<p>To learn more about the PyTorch API, you can start by reading the trial definitions from the
following examples:</p>
<ul class="simple">
<li><p><code class="xref download docutils literal notranslate"><span class="pre">cifar10_pytorch.tgz</span></code></p></li>
<li><p><code class="xref download docutils literal notranslate"><span class="pre">mnist_pytorch.tgz</span></code></p></li>
<li><p><code class="xref download docutils literal notranslate"><span class="pre">fasterrcnn_coco_pytorch.tgz</span></code></p></li>
</ul>
<p>For tips on debugging, see <a class="reference internal" href="../debug-models.html#model-debug"><span class="std std-ref">How to Debug Models</span></a>.</p>
<section id="download-data">
<span id="pytorch-downloading-data"></span><h3>Download Data<a class="headerlink" href="#download-data" title="Permalink to this heading">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before continuing, read how to <a class="reference internal" href="../load-model-data.html#prepare-data"><span class="std std-ref">Prepare Data</span></a> to understand how to work with different
sources of data.</p>
</div>
<p>There are two ways to download your dataset in the PyTorch API:</p>
<ol class="arabic simple">
<li><p>Download the data in the <a class="reference internal" href="../prepare-container/custom-env.html#startup-hooks"><span class="std std-ref">startup-hook.sh</span></a>.</p></li>
<li><p>Download the data in the constructor function <code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code>
of <code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrial</span></code>.</p></li>
</ol>
<p>If you are running a distributed training experiment, we suggest you to use the second approach.
During distributed training, a trial needs running multiple processes on different containers. In
order for all the processes to have access to the data and to prevent multiple download download
processes (one process per GPU) from conflicting with one another, the data should be downloaded to
unique directories for different ranks.</p>
<p>See the following code as an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">context</span>

    <span class="c1"># Create a unique download directory for each rank so they don&#39;t overwrite each</span>
    <span class="c1"># other when doing distributed training.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">download_directory</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;/tmp/data-rank</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">download_directory</span> <span class="o">=</span> <span class="n">download_data</span><span class="p">(</span>
       <span class="n">download_directory</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">download_directory</span><span class="p">,</span>
       <span class="n">url</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">get_data_config</span><span class="p">()[</span><span class="s2">&quot;url&quot;</span><span class="p">],</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="load-data">
<span id="pytorch-data-loading"></span><h3>Load Data<a class="headerlink" href="#load-data" title="Permalink to this heading">#</a></h3>
<p>Loading data into <code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrial</span></code> models is done by defining two
functions, <code class="xref py py-meth docutils literal notranslate"><span class="pre">build_training_data_loader()</span></code> and
<code class="xref py py-meth docutils literal notranslate"><span class="pre">build_validation_data_loader()</span></code>. Each function should return
an instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">determined.pytorch.DataLoader</span></code>.</p>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">determined.pytorch.DataLoader</span></code> class behaves the same as <code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code>
and is a drop-in replacement in most cases. It handles distributed training with
<code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrial</span></code>.</p>
<p>Each <code class="xref py py-class docutils literal notranslate"><span class="pre">determined.pytorch.DataLoader</span></code> will return batches of data, which will be fed directly
to the <code class="xref py py-meth docutils literal notranslate"><span class="pre">train_batch()</span></code> and
<code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_batch()</span></code> functions. The batch size of the data loader
will be set to the per-slot batch size, which is calculated based on <code class="docutils literal notranslate"><span class="pre">global_batch_size</span></code> and
<code class="docutils literal notranslate"><span class="pre">slots_per_trial</span></code> as defined in the <a class="reference internal" href="../../reference/training/experiment-config-reference.html#experiment-config-reference"><span class="std std-ref">experiment configuration</span></a>.</p>
<p>See the following code as an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">build_training_data_loader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">traindir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">download_directory</span><span class="p">,</span> <span class="s1">&#39;train&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">normalize</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                                <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>

    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span>
        <span class="n">traindir</span><span class="p">,</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
            <span class="n">transforms</span><span class="o">.</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
            <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
            <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">normalize</span><span class="p">,</span>
        <span class="p">]))</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">determined</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">train_dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">get_per_slot_batch_size</span><span class="p">(),</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">get_hparam</span><span class="p">(</span><span class="s2">&quot;workers&quot;</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">train_loader</span>
</pre></div>
</div>
<p>The output <code class="xref py py-meth docutils literal notranslate"><span class="pre">train_batch()</span></code> returns a batch of data in one of
the following formats:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># A numpy array</span>
<span class="n">batch</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="c1"># A PyTorch tensor</span>
<span class="n">batch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="c1"># A tuple of arrays or tensors</span>
<span class="n">batch</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
<span class="n">batch</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
<span class="c1"># A list of arrays or tensors</span>
<span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])]</span>
<span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])]</span>
<span class="c1"># A dictionary mapping strings to arrays or tensors</span>
<span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])}</span>
<span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])}</span>
<span class="c1"># A combination of the above</span>
<span class="n">batch</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;sub_data1&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])},</span>
        <span class="p">{</span><span class="s2">&quot;sub_data2&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])},</span>
    <span class="p">],</span>
    <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])),</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="initializing-objects">
<h3>Initializing Objects<a class="headerlink" href="#initializing-objects" title="Permalink to this heading">#</a></h3>
<p>You need to initialize the objects that will be used in training in the constructor
<code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code> of <code class="xref py py-class docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchTrial</span></code> using
the provided <code class="docutils literal notranslate"><span class="pre">context</span></code>: these objects include the model(s), optimizer(s), learning rate
scheduler(s), and custom loss and metric functions. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code> for details.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Be sure to wrap your objects! You may see metrics for trials that are paused and later continued
that are significantly different from trials that are not paused if some of your models,
optimizers, and learning rate schedulers are not wrapped. The reason is that the model’s state
may not be restored accurately or completely from the checkpoint, which is saved to a checkpoint
and then later loaded into the trial during resumed training. When using PyTorch, this can
sometimes happen if the PyTorch API is not used correctly.</p>
</div>
<section id="optimizers">
<h4>Optimizers<a class="headerlink" href="#optimizers" title="Permalink to this heading">#</a></h4>
<p>You need to call the <code class="xref py py-meth docutils literal notranslate"><span class="pre">wrap_optimizer()</span></code> method of the
<code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrialContext</span></code> to wrap your instantiated optimizers in the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code> constructor. For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">PyTorchTrialContext</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">context</span>

    <span class="o">...</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">get_hparam</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">),</span>
         <span class="n">momentum</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">get_hparam</span><span class="p">(</span><span class="s2">&quot;momentum&quot;</span><span class="p">),</span>
         <span class="n">weight_decay</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">get_hparam</span><span class="p">(</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">),</span>
     <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">wrap_optimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
<p>Then you need to step your optimizer in the <code class="xref py py-meth docutils literal notranslate"><span class="pre">train_batch()</span></code>
(see <a class="reference internal" href="#pytorch-optimization-step"><span class="std std-ref">Optimization Step</span></a> below).</p>
</section>
<section id="learning-rate-schedulers">
<h4>Learning Rate Schedulers<a class="headerlink" href="#learning-rate-schedulers" title="Permalink to this heading">#</a></h4>
<p>Determined has a few ways of managing the learning rate. Determined can automatically update every
batch or epoch, or you can manage it yourself.</p>
<p>You need to call the <code class="xref py py-meth docutils literal notranslate"><span class="pre">wrap_lr_scheduler()</span></code> method of the
<code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrialContext</span></code> to wrap your instantiated learning rate schedulers
in the <code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code> constructor. For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">PyTorchTrialContext</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">context</span>

    <span class="o">...</span>
    <span class="n">lr_sch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr_sch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">wrap_lr_scheduler</span><span class="p">(</span>
        <span class="n">lr_sch</span><span class="p">,</span>
        <span class="n">step_mode</span><span class="o">=</span><span class="n">LRScheduler</span><span class="o">.</span><span class="n">StepMode</span><span class="o">.</span><span class="n">STEP_EVERY_EPOCH</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>If your learning rate scheduler uses the manual step mode, you will need to step your learning rate
scheduler in the <code class="xref py py-meth docutils literal notranslate"><span class="pre">train_batch()</span></code> method of
<code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrial</span></code> by calling:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">pytorch</span><span class="o">.</span><span class="n">TorchData</span><span class="p">,</span> <span class="n">epoch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span>
    <span class="o">...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr_sch</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="o">...</span>
</pre></div>
</div>
</section>
</section>
<section id="define-the-training-loop">
<h3>Define the Training Loop<a class="headerlink" href="#define-the-training-loop" title="Permalink to this heading">#</a></h3>
<section id="optimization-step">
<span id="pytorch-optimization-step"></span><h4>Optimization Step<a class="headerlink" href="#optimization-step" title="Permalink to this heading">#</a></h4>
<p>You need to implement the <code class="xref py py-meth docutils literal notranslate"><span class="pre">train_batch()</span></code> method of your
<code class="docutils literal notranslate"><span class="pre">PyTorchTrial</span></code> subclass.</p>
<p>Typically when training with native PyTorch, you write a training loop, which iterates through the
dataloader to access and train your model one batch at a time. You can usually identify this code by
finding the common code snippet: <code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">batch</span> <span class="pre">in</span> <span class="pre">dataloader</span></code>. In Determined,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">train_batch()</span></code> also works with one batch at a time.</p>
<p>Take <a class="reference external" href="https://github.com/pytorch/examples/blob/master/imagenet/main.py">this script implemented with the native PyTorch</a> as an example. It has the
following code for the training loop.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="c1"># measure data loading time</span>
    <span class="n">data_time</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">end</span><span class="p">)</span>

    <span class="c1"># move data to the same device as model</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># compute output</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

    <span class="c1"># measure accuracy and record loss</span>
    <span class="n">acc1</span><span class="p">,</span> <span class="n">acc5</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">topk</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">images</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">top1</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">acc1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">images</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">top5</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">acc5</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">images</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

    <span class="c1"># compute gradient and do SGD step</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># measure elapsed time</span>
    <span class="n">batch_time</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">end</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">args</span><span class="o">.</span><span class="n">print_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">progress</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Notice that this pure-PyTorch loop manages the per-batch metrics. With Determined, metrics returned
by <code class="xref py py-meth docutils literal notranslate"><span class="pre">train_batch()</span></code> are automatically averaged and displayed, so
we do not need to do this ourselves.</p>
<p>Next, we will convert some PyTorch functions to use Determined’s equivalents. We need to change
<code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>, <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>, and <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>. The <code class="docutils literal notranslate"><span class="pre">self.context</span></code>
object will be used to call <code class="docutils literal notranslate"><span class="pre">loss.backwards</span></code> and handle zeroing and stepping the optimizer.</p>
<p>The final <code class="xref py py-meth docutils literal notranslate"><span class="pre">train_batch()</span></code> will look like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TorchData</span><span class="p">,</span> <span class="n">epoch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">images</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">acc1</span><span class="p">,</span> <span class="n">acc5</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">topk</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">step_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s2">&quot;top1&quot;</span><span class="p">:</span> <span class="n">acc1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;top5&quot;</span><span class="p">:</span> <span class="n">acc5</span><span class="p">[</span><span class="mi">0</span><span class="p">]}</span>
</pre></div>
</div>
</section>
<section id="checkpointing">
<h4>Checkpointing<a class="headerlink" href="#checkpointing" title="Permalink to this heading">#</a></h4>
<p>A checkpoint includes the model definition (Python source code), experiment configuration file,
network architecture, and the values of the model’s parameters (i.e., weights) and hyperparameters.
When using a stateful optimizer during training, checkpoints will also include the state of the
optimizer (i.e., learning rate). You can also embed arbitrary metadata in checkpoints via a
<a class="reference internal" href="../model-management/checkpoints.html#store-checkpoint-metadata"><span class="std std-ref">Python SDK</span></a>.</p>
<p>PyTorch trials are checkpointed as a <code class="docutils literal notranslate"><span class="pre">state-dict.pth</span></code> file. This file is created in a similar
manner to the procedure described in the <a class="reference external" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training">PyTorch documentation</a>,
but instead of the fields in that documentation, the dictionary will have four keys:
<code class="docutils literal notranslate"><span class="pre">models_state_dict</span></code>, <code class="docutils literal notranslate"><span class="pre">optimizers_state_dict</span></code>, <code class="docutils literal notranslate"><span class="pre">lr_schedulers_state_dict</span></code>, and <code class="docutils literal notranslate"><span class="pre">callbacks</span></code>,
which are the <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> of the models, optimizers, LR schedulers, and callbacks respectively.</p>
</section>
</section>
<section id="define-the-validation-loop">
<h3>Define the Validation Loop<a class="headerlink" href="#define-the-validation-loop" title="Permalink to this heading">#</a></h3>
<p>You need to implement either the <code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_batch()</span></code> or
<code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_full_dataset()</span></code> method. To load data into the
validation loop, define <code class="xref py py-meth docutils literal notranslate"><span class="pre">build_validation_data_loader()</span></code>. To
define reducing metrics, define <code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluation_reducer()</span></code>.</p>
<p>For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TorchData</span><span class="p">):</span>
    <span class="n">images</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">validation_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;validation_loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>
</pre></div>
</div>
</section>
<section id="callbacks">
<h3>Callbacks<a class="headerlink" href="#callbacks" title="Permalink to this heading">#</a></h3>
<p>To execute arbitrary Python code during the lifecycle of a
<code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrial</span></code>, implement the
<code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchCallback</span></code> and supply them to the
<code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrial</span></code> by implementing
<code class="xref py py-meth docutils literal notranslate"><span class="pre">build_callbacks()</span></code>.</p>
</section>
<section id="advanced-usage">
<h3>Advanced Usage<a class="headerlink" href="#advanced-usage" title="Permalink to this heading">#</a></h3>
<section id="gradient-clipping">
<h4>Gradient Clipping<a class="headerlink" href="#gradient-clipping" title="Permalink to this heading">#</a></h4>
<p>Users need to pass a gradient clipping function to
<code class="xref py py-meth docutils literal notranslate"><span class="pre">step_optimizer()</span></code>.</p>
</section>
<section id="reducing-metrics">
<span id="pytorch-custom-reducers"></span><h4>Reducing Metrics<a class="headerlink" href="#reducing-metrics" title="Permalink to this heading">#</a></h4>
<p>Determined supports proper reduction of arbitrary training and validation metrics, even during
distributed training, by allowing users to define custom reducers. Custom reducers can be either a
function or an implementation of the <code class="xref py py-class docutils literal notranslate"><span class="pre">determined.pytorch.MetricReducer</span></code> interface. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchTrialContext.wrap_reducer()</span></code> for more details.</p>
</section>
<section id="customize-a-reproducible-dataset">
<span id="pytorch-reproducible-dataset"></span><h4>Customize a Reproducible Dataset<a class="headerlink" href="#customize-a-reproducible-dataset" title="Permalink to this heading">#</a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Normally, using <code class="xref py py-class docutils literal notranslate"><span class="pre">determined.pytorch.DataLoader</span></code> is required and handles all of the below
details without any special effort on your part (see <a class="reference internal" href="#pytorch-data-loading"><span class="std std-ref">Load Data</span></a>). When
<code class="xref py py-class docutils literal notranslate"><span class="pre">determined.pytorch.DataLoader</span></code> is not suitable (especially in the case of
<code class="docutils literal notranslate"><span class="pre">IterableDatasets</span></code>), you may disable this requirement by calling
<code class="xref py py-meth docutils literal notranslate"><span class="pre">context.experimental.disable_dataset_reproducibility_checks()</span></code> in your
Trial’s <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method. Then you may choose to follow the below guidelines for ensuring
dataset reproducibility on your own.</p>
</div>
<p>Achieving a reproducible dataset that is able to pause and continue (sometimes called “incremental
training”) is easy if you follow a few rules.</p>
<ul>
<li><p>Even if you are going to ultimately return an IterableDataset, it is best to use PyTorch’s
Sampler class as the basis for choosing the order of records. Operations on Samplers are quick
and cheap, while operations on data afterwards are expensive. For more details, see the
discussion of random vs sequential access <a class="reference external" href="https://yogadl.readthedocs.io">here</a>. If you don’t
have a custom sampler, start with a simple one:</p>
</li>
<li><p><strong>Shuffle first</strong>: Always use a reproducible shuffle when you shuffle. Determined provides two
shuffling samplers for this purpose; the <code class="docutils literal notranslate"><span class="pre">ReproducibleShuffleSampler</span></code> for operating on records
and the <code class="docutils literal notranslate"><span class="pre">ReproducibleShuffleBatchSampler</span></code> for operating on batches. You should prefer to
shuffle on records (use the <code class="docutils literal notranslate"><span class="pre">ReproducibleShuffleSampler</span></code>) whenever possible, to achieve the
highest-quality shuffle.</p></li>
<li><p><strong>Repeat when training</strong>: In Determined, you always repeat your training dataset and you never
repeat your validation datasets. Determined provides a RepeatSampler and a RepeatBatchSampler to
wrap your sampler or batch_sampler. For your training dataset, make sure that you always repeat
AFTER you shuffle, otherwise your shuffle will hang.</p></li>
<li><p><strong>Always shard, and not before a repeat</strong>: Use Determined’s DistributedSampler or
DistributedBatchSampler to provide a unique shard of data to each worker based on your sampler or
batch_sampler. It is best to always shard your data, and even when you are not doing distributed
training, because in non-distributed-training settings, the sharding is nearly zero-cost, and it
makes distributed training seamless if you ever want to use it in the future.</p>
<p>It is generally important to shard after you repeat, unless you can guarantee that each shard of
the dataset will have the same length. Otherwise, differences between the epoch boundaries for
each worker can grow over time, especially on small datasets. If you shard after you repeat, you
can change the number of workers arbitrarily without issue.</p>
</li>
<li><p><strong>Skip when training, and always last</strong>: In Determined, training datasets should always be able
to start from an arbitrary point in the dataset. This allows for advanced hyperparameter searches
and responsive preemption for training on spot instances in the cloud. The easiest way to do
this, which is also very efficient, is to apply a skip to the sampler.</p>
<p>Determined provides a SkipBatchSampler that you can apply to your batch_sampler for this purpose.
There is also a SkipSampler that you can apply to your sampler, but you should prefer to skip on
batches unless you are confident that your dataset always yields identical size batches, where
the number of records to skip can be reliably calculated from the number of batches already
trained.</p>
<p>Always skip AFTER your repeat, so that the skip only happens once, and not on every epoch.</p>
<p>Always skip AFTER your shuffle, to preserve the reproducibility of the shuffle.</p>
</li>
</ul>
<p>Here is some example code that follows each of these rules that you can use as a starting point if
you find that the built-in context.DataLoader() does not support your use case.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_batch_sampler</span><span class="p">(</span>
  <span class="n">sampler_or_dataset</span><span class="p">,</span>
  <span class="n">mode</span><span class="p">,</span>  <span class="c1"># mode=&quot;training&quot; or mode=&quot;validation&quot;</span>
  <span class="n">shuffle_seed</span><span class="p">,</span>
  <span class="n">num_workers</span><span class="p">,</span>
  <span class="n">rank</span><span class="p">,</span>
  <span class="n">batch_size</span><span class="p">,</span>
  <span class="n">skip</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sampler_or_dataset</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Sampler</span><span class="p">):</span>
        <span class="n">sampler</span> <span class="o">=</span> <span class="n">sampler_or_dataset</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Create a SequentialSampler if we started with a Dataset.</span>
        <span class="n">sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">SequentialSampler</span><span class="p">(</span><span class="n">sampler_or_dataset</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;training&quot;</span><span class="p">:</span>
        <span class="c1"># Shuffle first.</span>
        <span class="n">sampler</span> <span class="o">=</span> <span class="n">samplers</span><span class="o">.</span><span class="n">ReproducibleShuffleSampler</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="n">shuffle_seed</span><span class="p">)</span>

        <span class="c1"># Repeat when training.</span>
        <span class="n">sampler</span> <span class="o">=</span> <span class="n">samplers</span><span class="o">.</span><span class="n">RepeatSampler</span><span class="p">(</span><span class="n">sampler</span><span class="p">)</span>

    <span class="c1"># Always shard, and not before a repeat.</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="n">samplers</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>

    <span class="c1"># Batch before skip, because Determined counts batches, not records.</span>
    <span class="n">batch_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">BatchSampler</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;training&quot;</span><span class="p">:</span>
        <span class="c1"># Skip when training, and always last.</span>
        <span class="n">batch_sampler</span> <span class="o">=</span> <span class="n">samplers</span><span class="o">.</span><span class="n">SkipBatchSampler</span><span class="p">(</span><span class="n">batch_sampler</span><span class="p">,</span> <span class="n">skip</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">batch_sampler</span>

<span class="k">class</span> <span class="nc">MyPyTorchTrial</span><span class="p">(</span><span class="n">det</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">PyTorchTrial</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="n">context</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">disable_dataset_reproducibility_checks</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">build_training_data_loader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">my_dataset</span> <span class="o">=</span> <span class="o">...</span>

        <span class="n">batch_sampler</span> <span class="o">=</span> <span class="n">make_batch_sampler</span><span class="p">(</span>
            <span class="n">dataset</span><span class="o">=</span><span class="n">my_dataset</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">get_trial_seed</span><span class="p">(),</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_size</span><span class="p">(),</span>
            <span class="n">rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(),</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">get_per_slot_batch_size</span><span class="p">(),</span>
            <span class="n">skip</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">get_initial_batch</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">my_dataset</span><span class="p">,</span> <span class="n">batch_sampler</span><span class="o">=</span><span class="n">batch_sampler</span><span class="p">)</span>
</pre></div>
</div>
<p>See the <code class="xref py py-mod docutils literal notranslate"><span class="pre">determined.pytorch.samplers</span></code> for details.</p>
</section>
<section id="profiling">
<h4>Profiling<a class="headerlink" href="#profiling" title="Permalink to this heading">#</a></h4>
<p>Determined provides support for the native PyTorch profiler, <a class="reference external" href="https://github.com/pytorch/kineto/tree/main/tb_plugin">torch-tb-profiler</a>. You can configure this by calling
<code class="xref py py-meth docutils literal notranslate"><span class="pre">set_profiler()</span></code> from within your Trial’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code>.
<code class="docutils literal notranslate"><span class="pre">set_profiler</span></code> accepts the same arguments as the PyTorch plugin’s <code class="docutils literal notranslate"><span class="pre">torch.profiler.profile</span></code>
method. However, Determined sets <code class="docutils literal notranslate"><span class="pre">on_trace_ready</span></code> to the appropriate TensorBoard path, and the
stepping of the profiler during training is automatically handled.</p>
<p>The following example profiles CPU and GPU activities on batches 3 and 4 (skipping batch 1, warming
up on batch 2), and repeats for 2 cycles:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyPyTorchTrial</span><span class="p">(</span><span class="n">det</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">PyTorchTrial</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="n">context</span><span class="o">.</span><span class="n">set_profiler</span><span class="p">(</span>
            <span class="n">activities</span><span class="o">=</span><span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="n">schedule</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">schedule</span><span class="p">(</span>
                <span class="n">wait</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">warmup</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">active</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">repeat</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
</pre></div>
</div>
<p>See the <a class="reference external" href="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html#use-profiler-to-record-execution-events">PyTorch tensorboard profiler tutorial</a>
for a complete list of accepted configurations parameters.</p>
</section>
</section>
<section id="porting-checklist">
<h3>Porting Checklist<a class="headerlink" href="#porting-checklist" title="Permalink to this heading">#</a></h3>
<p>If you port your code to Determined, you should walk through this checklist to ensure your code does
not conflict with the Determined library.</p>
<section id="remove-pinned-gpus">
<h4>Remove Pinned GPUs<a class="headerlink" href="#remove-pinned-gpus" title="Permalink to this heading">#</a></h4>
<p>Determined handles scheduling jobs on available slots. However, you need to let the Determined
library handles choosing the GPUs.</p>
<p>Take <a class="reference external" href="https://github.com/pytorch/examples/blob/master/imagenet/main.py">this script</a> as an
example. It has the following code to configure the GPU:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">gpu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Use GPU: </span><span class="si">{}</span><span class="s2"> for training&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpu</span><span class="p">))</span>
</pre></div>
</div>
<p>Any use of <code class="docutils literal notranslate"><span class="pre">args.gpu</span></code> should be removed.</p>
</section>
<section id="remove-distributed-training-code">
<h4>Remove Distributed Training Code<a class="headerlink" href="#remove-distributed-training-code" title="Permalink to this heading">#</a></h4>
<p>To run distributed training outside Determined, you need to have code that handles the logic of
launching processes, moving models to pined GPUs, sharding data, and reducing metrics. You need to
remove this code to be not conflict with the Determined library.</p>
<p>Take <a class="reference external" href="https://github.com/pytorch/examples/blob/master/imagenet/main.py">this script</a> as an
example. It has the following code to initialize the process group:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">dist_url</span> <span class="o">==</span> <span class="s2">&quot;env://&quot;</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RANK&quot;</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">multiprocessing_distributed</span><span class="p">:</span>
        <span class="c1"># For multiprocessing distributed training, rank needs to be the</span>
        <span class="c1"># global rank among all the processes</span>
        <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">*</span> <span class="n">ngpus_per_node</span> <span class="o">+</span> <span class="n">gpu</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dist_backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dist_url</span><span class="p">,</span>
                            <span class="n">world_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">)</span>
</pre></div>
</div>
<p>This example also has the following code to set up CUDA and converts the model to a distributed one.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;using CPU, this will be slow&#39;</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
    <span class="c1"># For multiprocessing distributed, DistributedDataParallel constructor</span>
    <span class="c1"># should always set the single device scope, otherwise,</span>
    <span class="c1"># DistributedDataParallel will use all available devices.</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">gpu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpu</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpu</span><span class="p">)</span>
        <span class="c1"># When using a single GPU per process and per</span>
        <span class="c1"># DistributedDataParallel, we need to divide the batch size</span>
        <span class="c1"># ourselves based on the total number of GPUs we have</span>
        <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">/</span> <span class="n">ngpus_per_node</span><span class="p">)</span>
        <span class="n">args</span><span class="o">.</span><span class="n">workers</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">args</span><span class="o">.</span><span class="n">workers</span> <span class="o">+</span> <span class="n">ngpus_per_node</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">ngpus_per_node</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">gpu</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="c1"># DistributedDataParallel will divide and allocate batch_size to all</span>
        <span class="c1"># available GPUs if device_ids are not set</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">gpu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpu</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpu</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># DataParallel will divide and allocate batch_size to all available GPUs</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">arch</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;alexnet&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">arch</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;vgg&#39;</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>
</div>
<p>This code is unnecessary in the trial definition. When we create the model, we will wrap it with
<code class="docutils literal notranslate"><span class="pre">self.context.wrap_model(model)</span></code>, which will convert the model to distributed if needed. We will
also automatically set up horovod for you. If you would like to access the rank (typically used to
view per GPU training), you can get it by calling <code class="docutils literal notranslate"><span class="pre">self.context.distributed.rank</span></code>.</p>
<p>To handle data loading in distributed training, this example has the code below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">traindir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">valdir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">)</span>
<span class="n">normalize</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                                <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span>
    <span class="n">traindir</span><span class="p">,</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">normalize</span><span class="p">,</span>
    <span class="p">]))</span>

<span class="c1"># Handle distributed sampler for distributed training.</span>
<span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">train_sampler</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
<p>This should be removed since we will use distributed data loader if you following the instructions
of <code class="xref py py-meth docutils literal notranslate"><span class="pre">build_training_data_loader()</span></code> and
<code class="xref py py-meth docutils literal notranslate"><span class="pre">build_validation_data_loader()</span></code>.</p>
</section>
<section id="get-hyperparameters-from-pytorchtrialcontext">
<h4>Get Hyperparameters from PyTorchTrialContext<a class="headerlink" href="#get-hyperparameters-from-pytorchtrialcontext" title="Permalink to this heading">#</a></h4>
<p>Take the following code for example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">PyTorchTrialContext</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">context</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">pretrained</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&gt; using pre-trained model &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">arch</span><span class="p">))</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">arch</span><span class="p">](</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&gt; creating model &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">arch</span><span class="p">))</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">arch</span><span class="p">]()</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">args.arch</span></code> is a hyperparameter. You should define the hyperparameter space in the
<a class="reference internal" href="../../reference/training/experiment-config-reference.html#experiment-config-reference"><span class="std std-ref">experiment config</span></a>. By doing so, you get better tracking in the
WebUI, especially for experiments that use a searcher. Depending on how your trial is run, you can
access all the current hyperparameters from inside the trial by either calling
<code class="docutils literal notranslate"><span class="pre">self.context.get_hparams()</span></code> if you submitted your trial with <code class="docutils literal notranslate"><span class="pre">entrypoint:</span> <span class="pre">model_def:Trial</span></code> or
passing in hyperparameters directly into the Trial <code class="docutils literal notranslate"><span class="pre">__init__</span></code> if using PyTorch Trainer API.</p>
</section>
</section>
</section>
<section id="pytorch-trainer">
<span id="pytorch-trainer-ug"></span><h2>PyTorch Trainer<a class="headerlink" href="#pytorch-trainer" title="Permalink to this heading">#</a></h2>
<p>With the PyTorch Trainer API, you can implement and iterate on model training code locally before
running on cluster. When you are satisfied with your model code, you configure and submit the code
on cluster.</p>
<p>The PyTorch Trainer API lets you do the following:</p>
<ul class="simple">
<li><p>Work locally, iterating on your model code.</p></li>
<li><p>Debug models in your favorite debug environment (e.g., directly on your machine, IDE, or Jupyter
notebook).</p></li>
<li><p>Run training scripts without needing to use an experiment configuration file.</p></li>
<li><p>Load previously saved checkpoints directly into your model.</p></li>
</ul>
<section id="initializing-the-trainer">
<h3>Initializing the Trainer<a class="headerlink" href="#initializing-the-trainer" title="Permalink to this heading">#</a></h3>
<p>After defining the PyTorch Trial, initialize the trial and the trainer.
<code class="xref py py-meth docutils literal notranslate"><span class="pre">init()</span></code> returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrialContext</span></code> for
instantiating <code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrial</span></code>. Initialize
<code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> with the trial and context.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">determined</span> <span class="kn">import</span> <span class="n">pytorch</span>
<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">det</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">init</span><span class="p">()</span> <span class="k">as</span> <span class="n">train_context</span><span class="p">:</span>
        <span class="n">trial</span> <span class="o">=</span> <span class="n">MyTrial</span><span class="p">(</span><span class="n">train_context</span><span class="p">)</span>
        <span class="n">trainer</span> <span class="o">=</span> <span class="n">det</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="n">train_context</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Configure logging</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="n">det</span><span class="o">.</span><span class="n">LOG_FORMAT</span><span class="p">)</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p>Training is configured with a call to <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code> with training loop
arguments, such as checkpointing periods, validation periods, and checkpointing policy.</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span>from determined import pytorch


def main():
<span class="w"> </span>   with det.pytorch.init() as train_context:
<span class="w"> </span>       trial = MyTrial(train_context)
<span class="w"> </span>       trainer = det.pytorch.Trainer(trial, train_context)
<span class="gi">+       trainer.fit(</span>
<span class="gi">+           checkpoint_period=pytorch.Batch(100),</span>
<span class="gi">+           validation_period=pytorch.Batch(100),</span>
<span class="gi">+           checkpoint_policy=&quot;all&quot;</span>
<span class="gi">+       )</span>


if __name__ == &quot;__main__&quot;:
<span class="w"> </span>   # Configure logging
<span class="w"> </span>   logging.basicConfig(level=logging.INFO, format=det.LOG_FORMAT)
<span class="w"> </span>   main()
</pre></div>
</div>
</section>
<section id="run-your-training-script-locally">
<h3>Run Your Training Script Locally<a class="headerlink" href="#run-your-training-script-locally" title="Permalink to this heading">#</a></h3>
<p>Run training scripts locally without submitting to a cluster or defining an experiment configuration
file. Be sure to specify <code class="docutils literal notranslate"><span class="pre">max_length</span></code> in the <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> call, which is used in local training mode
to determine the maximum number of steps to train for.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">determined</span> <span class="kn">import</span> <span class="n">pytorch</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">det</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">init</span><span class="p">()</span> <span class="k">as</span> <span class="n">train_context</span><span class="p">:</span>
        <span class="n">trial</span> <span class="o">=</span> <span class="n">MyTrial</span><span class="p">(</span><span class="n">train_context</span><span class="p">)</span>
        <span class="n">trainer</span> <span class="o">=</span> <span class="n">det</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="n">train_context</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">pytorch</span><span class="o">.</span><span class="n">Epoch</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">checkpoint_period</span><span class="o">=</span><span class="n">pytorch</span><span class="o">.</span><span class="n">Batch</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
            <span class="n">validation_period</span><span class="o">=</span><span class="n">pytorch</span><span class="o">.</span><span class="n">Batch</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
            <span class="n">checkpoint_policy</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Configure logging</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="n">det</span><span class="o">.</span><span class="n">LOG_FORMAT</span><span class="p">)</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p>You can run this Python script directly (<code class="docutils literal notranslate"><span class="pre">python3</span> <span class="pre">train.py</span></code>), or in a Jupyter notebook. This code
will train for one epoch, and checkpoint and validate every 100 batches.</p>
</section>
<section id="local-distributed-training">
<h3>Local Distributed Training<a class="headerlink" href="#local-distributed-training" title="Permalink to this heading">#</a></h3>
<p>Local training can utilize multiple GPUs on a single node with a few modifications to the above
code. Both Horovod and PyTorch Distributed backends are supported.</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>def main():
<span class="gi">+     # Initialize distributed backend before pytorch.init()</span>
<span class="gi">+     dist.init_process_group(backend=&quot;gloo|nccl&quot;)</span>
<span class="gi">+     # Set flag used by internal PyTorch training loop</span>
<span class="gi">+     os.environ[&quot;USE_TORCH_DISTRIBUTED&quot;] = &quot;true&quot;</span>
<span class="gi">+     # Initialize DistributedContext</span>
<span class="w"> </span>     with det.pytorch.init(
<span class="gi">+       distributed=core.DistributedContext.from_torch_distributed()</span>
<span class="w"> </span>     ) as train_context:
<span class="w"> </span>         trial = MyTrial(train_context)
<span class="w"> </span>         trainer = det.pytorch.Trainer(trial, train_context)
<span class="w"> </span>         trainer.fit(
<span class="w"> </span>             max_length=pytorch.Epoch(1),
<span class="w"> </span>             checkpoint_period=pytorch.Batch(100),
<span class="w"> </span>             validation_period=pytorch.Batch(100),
<span class="w"> </span>             checkpoint_policy=&quot;all&quot;
<span class="w"> </span>         )
</pre></div>
</div>
<p>This code can be directly invoked with your distributed backend’s launcher: <code class="docutils literal notranslate"><span class="pre">torchrun</span>
<span class="pre">--nproc_per_node=4</span> <span class="pre">train.py</span></code></p>
</section>
<section id="test-mode">
<h3>Test Mode<a class="headerlink" href="#test-mode" title="Permalink to this heading">#</a></h3>
<p>Trainer accepts a test_mode parameter which, if true, trains and validates your training code for
only one batch, checkpoints, then exits. This is helpful for debugging code or writing automated
tests around your model code.</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>trainer.fit(
<span class="w"> </span>             max_length=pytorch.Epoch(1),
<span class="w"> </span>             checkpoint_period=pytorch.Batch(100),
<span class="w"> </span>             validation_period=pytorch.Batch(100),
<span class="gi">+             test_mode=True</span>
<span class="w"> </span>         )
</pre></div>
</div>
</section>
<section id="prepare-your-training-code-for-deploying-to-a-determined-cluster">
<h3>Prepare Your Training Code for Deploying to a Determined Cluster<a class="headerlink" href="#prepare-your-training-code-for-deploying-to-a-determined-cluster" title="Permalink to this heading">#</a></h3>
<p>Once you are satisfied with the results of training the model locally, you submit the code to a
cluster. This example allows for distributed training locally and on cluster without having to make
code changes.</p>
<p>Example workflow of frequent iterations between local debugging and cluster deployment:</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>def main():
<span class="gi">+   local = det.get_cluster_info() is None</span>
<span class="gi">+   if local:</span>
<span class="gi">+       # Local: configure local distributed training.</span>
<span class="gi">+       dist.init_process_group(backend=&quot;gloo|nccl&quot;)</span>
<span class="gi">+       os.environ[&quot;USE_TORCH_DISTRIBUTED&quot;] = &quot;true&quot;</span>
<span class="gi">+       distributed_context = core.DistributedContext.from_torch_distributed()</span>
<span class="gi">+       latest_checkpoint = None</span>
<span class="gi">+   else:</span>
<span class="gi">+       # On-cluster: Determined will automatically detect distributed context.</span>
<span class="gi">+       distributed_context = None</span>
<span class="gi">+       # On-cluster: configure the latest checkpoint for pause/resume training functionality.</span>
<span class="gi">+       latest_checkpoint = det.get_cluster_info().latest_checkpoint</span>

<span class="gi">+     with det.pytorch.init(</span>
<span class="gi">+       distributed=distributed_context</span>
<span class="w"> </span>     ) as train_context:
<span class="w"> </span>         trial = MNistTrial(train_context)
<span class="w"> </span>         trainer = det.pytorch.Trainer(trial, train_context)
<span class="w"> </span>         trainer.fit(
<span class="w"> </span>             max_length=pytorch.Epoch(1),
<span class="w"> </span>             checkpoint_period=pytorch.Batch(100),
<span class="w"> </span>             validation_period=pytorch.Batch(100),
<span class="gi">+             latest_checkpoint=latest_checkpoint,</span>
<span class="w"> </span>         )
</pre></div>
</div>
<p>To run Trainer API solely on-cluster, the code is much simpler:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">det</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">init</span><span class="p">()</span> <span class="k">as</span> <span class="n">train_context</span><span class="p">:</span>
        <span class="n">trial_inst</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">MNistTrial</span><span class="p">(</span><span class="n">train_context</span><span class="p">)</span>
        <span class="n">trainer</span> <span class="o">=</span> <span class="n">det</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">trial_inst</span><span class="p">,</span> <span class="n">train_context</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
            <span class="n">checkpoint_period</span><span class="o">=</span><span class="n">pytorch</span><span class="o">.</span><span class="n">Batch</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
            <span class="n">validation_period</span><span class="o">=</span><span class="n">pytorch</span><span class="o">.</span><span class="n">Batch</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
            <span class="n">latest_checkpoint</span><span class="o">=</span><span class="n">det</span><span class="o">.</span><span class="n">get_cluster_info</span><span class="p">()</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">,</span>
        <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="submit-your-trial-for-training-on-cluster">
<h3>Submit Your Trial for Training on Cluster<a class="headerlink" href="#submit-your-trial-for-training-on-cluster" title="Permalink to this heading">#</a></h3>
<p>To run your experiment on cluster, you’ll need to create an experiment configuration (YAML) file.
Your experiment configuration file must contain searcher configuration and entrypoint.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">name</span><span class="p">:</span> <span class="n">pytorch_trainer_trial</span>
<span class="n">searcher</span><span class="p">:</span>
  <span class="n">name</span><span class="p">:</span> <span class="n">single</span>
  <span class="n">metric</span><span class="p">:</span> <span class="n">validation_loss</span>
  <span class="n">max_length</span><span class="p">:</span>
    <span class="n">epochs</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">resources</span><span class="p">:</span>
  <span class="n">slots_per_trial</span><span class="p">:</span> <span class="mi">8</span>
<span class="n">entrypoint</span><span class="p">:</span> <span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">determined</span><span class="o">.</span><span class="n">launch</span><span class="o">.</span><span class="n">torch_distributed</span> <span class="n">python3</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>Submit the trial to the cluster:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>det<span class="w"> </span>e<span class="w"> </span>create<span class="w"> </span>det.yaml<span class="w"> </span>.
</pre></div>
</div>
<p>If your training code needs to read some values from the experiment configuration,
<code class="docutils literal notranslate"><span class="pre">pytorch.init()</span></code> accepts an <code class="docutils literal notranslate"><span class="pre">exp_conf</span></code> argument which allows calling
<code class="docutils literal notranslate"><span class="pre">context.get_experiment_config()</span></code> from <code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext</span></code>.</p>
</section>
<section id="loading-checkpoints">
<h3>Loading Checkpoints<a class="headerlink" href="#loading-checkpoints" title="Permalink to this heading">#</a></h3>
<p>To load a checkpoint from a checkpoint saved using Trainer, you’ll need to download the checkpoint
to a file directory and use <code class="xref py py-func docutils literal notranslate"><span class="pre">determined.pytorch.load_trial_from_checkpoint_path()</span></code>. If your
<code class="docutils literal notranslate"><span class="pre">Trial</span></code> was instantiated with arguments, you can pass them via the <code class="docutils literal notranslate"><span class="pre">trial_kwargs</span></code> parameter of
<code class="docutils literal notranslate"><span class="pre">load_trial_from_checkpoint_path</span></code>.</p>
</section>
</section>
</section>


                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="api-core-ug.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Core API User Guide</p>
      </div>
    </a>
    <a class="right-next"
       href="api-keras-ug.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Keras API</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-trial">PyTorch Trial</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#download-data">Download Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-data">Load Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initializing-objects">Initializing Objects</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizers">Optimizers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-schedulers">Learning Rate Schedulers</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-training-loop">Define the Training Loop</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-step">Optimization Step</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpointing">Checkpointing</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-validation-loop">Define the Validation Loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#callbacks">Callbacks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-usage">Advanced Usage</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-clipping">Gradient Clipping</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reducing-metrics">Reducing Metrics</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#customize-a-reproducible-dataset">Customize a Reproducible Dataset</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#profiling">Profiling</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#porting-checklist">Porting Checklist</a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#remove-pinned-gpus">Remove Pinned GPUs</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#remove-distributed-training-code">Remove Distributed Training Code</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#get-hyperparameters-from-pytorchtrialcontext">Get Hyperparameters from PyTorchTrialContext</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-trainer">PyTorch Trainer</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initializing-the-trainer">Initializing the Trainer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#run-your-training-script-locally">Run Your Training Script Locally</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#local-distributed-training">Local Distributed Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-mode">Test Mode</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-your-training-code-for-deploying-to-a-determined-cluster">Prepare Your Training Code for Deploying to a Determined Cluster</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#submit-your-trial-for-training-on-cluster">Submit Your Trial for Training on Cluster</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-checkpoints">Loading Checkpoints</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Tara & LB
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023, Determined AI.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>