
<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />
<meta content="Familiarize yourself with the det.pytorch API. This PyTorch-based training loop includes the PyTorchTrial class, the PyTorchTrialContext class, and the Trainer class." name="description" />

    <title>det.pytorch API Reference &#8212; project-x 0.0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=362ab14a" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js?v=dc820ae5"></script>
    <script src="../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'latest/reference/training/api-pytorch-reference';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
    <p class="title logo__title">project-x 0.0.1 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Welcome to project-x’s documentation!
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../example-solutions/_index.html">Examples</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../integrations/_index.html">Integrations</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../integrations/ecosystem/_index.html">Ecosystem Integration</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../integrations/notification/_index.html">Monitoring Experiment Through Webhooks</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../integrations/notification/zapier.html">Through Zapier</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../integrations/notification/slack.html">Through Slack</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../integrations/prometheus/_index.html">Configure Determined with Prometheus and Grafana</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../model-dev-guide/_index.html">Model Developer Guide</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../model-dev-guide/apis-howto/_index.html">Training APIs</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../model-dev-guide/apis-howto/api-core-ug.html">Core API User Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../model-dev-guide/apis-howto/api-pytorch-ug.html">PyTorch API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../model-dev-guide/apis-howto/api-keras-ug.html">Keras API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../model-dev-guide/apis-howto/api-estimator-ug.html">Estimator API</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../model-dev-guide/batch-processing/_index.html">Torch Batch Processing API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../model-dev-guide/best-practices/_index.html">Best Practices</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../model-dev-guide/dtrain/_index.html">Distributed Training with Determined</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../model-dev-guide/dtrain/dtrain-introduction.html">Distributed Training Concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../model-dev-guide/dtrain/dtrain-implement.html">Implementing Distributed Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../model-dev-guide/dtrain/config-templates.html">Configuration Templates</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../model-dev-guide/dtrain/reproducibility.html">Reproducibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../model-dev-guide/dtrain/optimize-training.html">Optimizing Training</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../model-hub-library/_index.html">Model Hub Library</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../_index.html">Reference</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../release-notes/_index.html">Release Notes</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul class="simple">
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../setup-cluster/_index.html">Setup</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/_index.html">Set Up Determined</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/aws/_index.html">Deploy on AWS</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/aws/aws-spot.html">Use Spot Instances</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/aws/dynamic-agents-aws.html">Deploy Determined with Dynamic Agents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/aws/install-on-aws.html">Install Determined</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/gcp/_index.html">Deploy on GCP</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/gcp/dynamic-agents-gcp.html">Deploy Determined with Dynamic Agents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/gcp/install-gcp.html">Install Determined</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/_index.html">Deploy on Kubernetes</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/custom-pod-specs.html">Customize a Pod</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/helm-commands.html">Helm and Kubectl Command Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/install-on-kubernetes.html">Install Determined on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/k8s-dev-guide.html">Development Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/setup-aks-cluster.html">Set up and Manage an Azure Kubernetes Service (AKS) Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/setup-eks-cluster.html">Set up and Manage an AWS Kubernetes (EKS) Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/setup-gke-cluster.html">Set up and Manage a Google Kubernetes Engine (GKE) Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/k8s/troubleshooting.html">Troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/_index.html">Deploy on Prem</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/deploy.html">Install Determined Using <code class="docutils literal notranslate"><span class="pre">det</span> <span class="pre">deploy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/docker.html">Install Determined Using Docker</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/homebrew.html">Install Determined Using Homebrew (macOS)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/linux-packages.html">Install Determined Using Linux Packages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/requirements.html">Installation Requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/on-prem/wsl.html">Install Determined Using Windows Subsystem for Linux (Windows)</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/_index.html">Deploy on Slurm/PBS</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/hpc-launching-architecture.html">HPC Launching Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/hpc-security-considerations.html">HPC Launcher Security Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/install-on-slurm.html">Install Determined on Slurm/PBS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/singularity.html">Provide a Container Image Cache</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/slurm-known-issues.html">Known Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../setup-cluster/deploy-cluster/slurm/slurm-requirements.html">Installation Requirements</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../setup-cluster/security/_index.html">Security</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/oauth.html">OAuth 2.0 Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/oidc.html">OpenID Connect Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/rbac.html">RBAC</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/saml.html">SAML Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/scim.html">SCIM Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../setup-cluster/security/tls.html">Transport Layer Security</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/_index.html">Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/pytorch-mnist-local-qs.html">Run Your First Experiment in Determined</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/pytorch-mnist-tutorial.html">PyTorch MNIST Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/pytorch-porting-tutorial.html">PyTorch Porting Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/quickstart-mdldev.html">Quickstart for Model Developers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/tf-mnist-tutorial.html">TensorFlow Keras Fashion MNIST Tutorial</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/latest/reference/training/api-pytorch-reference.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>det.pytorch API Reference</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> {your-title} </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-pytorchtrial"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchTrial</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.trial_context_class"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.trial_context_class</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.__init__"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.__init__()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.train_batch"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.train_batch()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.build_training_data_loader"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.build_training_data_loader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.build_validation_data_loader"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.build_validation_data_loader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.build_callbacks"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.build_callbacks()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.evaluate_batch"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.evaluate_batch()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.evaluation_reducer"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.evaluation_reducer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.evaluate_full_dataset"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.evaluate_full_dataset()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.get_batch_length"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.get_batch_length()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-pytorchtrialcontext"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchTrialContext</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.backward"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.configure_apex_amp"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.configure_apex_amp()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.current_train_batch"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.current_train_batch()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_data_config"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_data_config()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_experiment_id"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_experiment_id()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_global_batch_size"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_global_batch_size()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_hparam"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_hparam()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_per_slot_batch_size"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_per_slot_batch_size()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_stop_requested"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_stop_requested()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_tensorboard_path"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_tensorboard_path()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_tensorboard_writer"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_tensorboard_writer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_trial_id"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_trial_id()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.is_epoch_end"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.is_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.is_epoch_start"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.is_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.set_profiler"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.set_profiler()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.set_stop_requested"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.set_stop_requested()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.step_optimizer"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.step_optimizer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.to_device"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.to_device()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.wrap_lr_scheduler"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.wrap_lr_scheduler()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.wrap_model"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.wrap_model()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.wrap_optimizer"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.wrap_optimizer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.wrap_scaler"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.wrap_scaler()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-pytorchtrialcontext-distributed"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchTrialContext.distributed</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-pytorchexperimentalcontext"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchExperimentalContext</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchExperimentalContext"><code class="docutils literal notranslate"><span class="pre">PyTorchExperimentalContext</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchExperimentalContext.disable_auto_to_device"><code class="docutils literal notranslate"><span class="pre">PyTorchExperimentalContext.disable_auto_to_device()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchExperimentalContext.disable_dataset_reproducibility_checks"><code class="docutils literal notranslate"><span class="pre">PyTorchExperimentalContext.disable_dataset_reproducibility_checks()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchExperimentalContext.use_amp"><code class="docutils literal notranslate"><span class="pre">PyTorchExperimentalContext.use_amp()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-dataloader"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.DataLoader</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.DataLoader"><code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-lrscheduler"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.LRScheduler</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.LRScheduler"><code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.LRScheduler.StepMode"><code class="docutils literal notranslate"><span class="pre">LRScheduler.StepMode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.LRScheduler.__init__"><code class="docutils literal notranslate"><span class="pre">LRScheduler.__init__()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-reducer"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.Reducer</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.Reducer"><code class="docutils literal notranslate"><span class="pre">Reducer</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-metricreducer"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.MetricReducer</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.MetricReducer"><code class="docutils literal notranslate"><span class="pre">MetricReducer</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.MetricReducer.reset"><code class="docutils literal notranslate"><span class="pre">MetricReducer.reset()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.MetricReducer.per_slot_reduce"><code class="docutils literal notranslate"><span class="pre">MetricReducer.per_slot_reduce()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.MetricReducer.cross_slot_reduce"><code class="docutils literal notranslate"><span class="pre">MetricReducer.cross_slot_reduce()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-pytorchcallback"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchCallback</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.load_state_dict"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.load_state_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_checkpoint_end"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_checkpoint_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_checkpoint_load_start"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_checkpoint_load_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_checkpoint_save_start"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_checkpoint_save_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_checkpoint_upload_end"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_checkpoint_upload_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_checkpoint_write_end"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_checkpoint_write_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_training_epoch_end"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_training_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_training_epoch_start"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_training_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_training_start"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_training_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_training_workload_end"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_training_workload_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_trial_shutdown"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_trial_shutdown()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_trial_startup"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_trial_startup()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_validation_end"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_validation_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_validation_epoch_end"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_validation_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_validation_epoch_start"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_validation_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_validation_start"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_validation_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.state_dict"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.state_dict()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-load-trial-from-checkpoint-path"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.load_trial_from_checkpoint_path</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.load_trial_from_checkpoint_path"><code class="docutils literal notranslate"><span class="pre">load_trial_from_checkpoint_path()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-trainer"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.Trainer</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.Trainer"><code class="docutils literal notranslate"><span class="pre">Trainer</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.Trainer.configure_profiler"><code class="docutils literal notranslate"><span class="pre">Trainer.configure_profiler()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.Trainer.fit"><code class="docutils literal notranslate"><span class="pre">Trainer.fit()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-init"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.init()</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.init"><code class="docutils literal notranslate"><span class="pre">init()</span></code></a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="det-pytorch-api-reference">
<h1><code class="docutils literal notranslate"><span class="pre">det.pytorch</span></code> API Reference<a class="headerlink" href="#det-pytorch-api-reference" title="Permalink to this heading">#</a></h1>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>User Guide</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="../../model-dev-guide/apis-howto/api-pytorch-ug.html#pytorch-trial-ug"><span class="std std-ref">PyTorch Trial</span></a></p></td>
</tr>
</tbody>
</table>
<p>Determined offers a PyTorch-based training loop that is fully integrated with the Determined
platform which includes:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#determined.pytorch.PyTorchTrial" title="determined.pytorch.PyTorchTrial"><code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrial</span></code></a>, which you must subclass to define things like model
architecture, optimizer, data loaders, and how to train or validate a single batch.</p></li>
<li><p><a class="reference internal" href="#determined.pytorch.PyTorchTrialContext" title="determined.pytorch.PyTorchTrialContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrialContext</span></code></a>, which can be accessed from within
<code class="docutils literal notranslate"><span class="pre">PyTorchTrial</span></code> and contains runtime methods used for training with the <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> API.</p></li>
<li><p><a class="reference internal" href="#determined.pytorch.Trainer" title="determined.pytorch.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>, which is used for customizing and executing the training
loop around a <code class="docutils literal notranslate"><span class="pre">PyTorchTrial</span></code>.</p></li>
</ul>
<section id="determined-pytorch-pytorchtrial">
<span id="pytorch-api-ref"></span><h2><code class="docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchTrial</span></code><a class="headerlink" href="#determined-pytorch-pytorchtrial" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrial">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">determined.pytorch.</span></span><span class="sig-name descname"><span class="pre">PyTorchTrial</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#determined.pytorch.PyTorchTrialContext" title="determined.pytorch._pytorch_context.PyTorchTrialContext"><span class="pre">PyTorchTrialContext</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#determined.pytorch.PyTorchTrial" title="Permalink to this definition">#</a></dt>
<dd><p>PyTorch trials are created by subclassing this abstract class.</p>
<p>We can do the following things in this trial class:</p>
<ul>
<li><p><strong>Define models, optimizers, and LR schedulers</strong>.</p>
<p>In the <a class="reference internal" href="#determined.pytorch.PyTorchTrial.__init__" title="determined.pytorch.PyTorchTrial.__init__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code></a> method, initialize models, optimizers, and LR schedulers
and wrap them with <code class="docutils literal notranslate"><span class="pre">wrap_model</span></code>, <code class="docutils literal notranslate"><span class="pre">wrap_optimizer</span></code>, <code class="docutils literal notranslate"><span class="pre">wrap_lr_scheduler</span></code>
provided by <a class="reference internal" href="#determined.pytorch.PyTorchTrialContext" title="determined.pytorch.PyTorchTrialContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrialContext</span></code></a>.</p>
</li>
<li><p><strong>Run forward and backward passes</strong>.</p>
<p>In <a class="reference internal" href="#determined.pytorch.PyTorchTrial.train_batch" title="determined.pytorch.PyTorchTrial.train_batch"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_batch()</span></code></a>, call <code class="docutils literal notranslate"><span class="pre">backward</span></code> and <code class="docutils literal notranslate"><span class="pre">step_optimizer</span></code> provided by
<a class="reference internal" href="#determined.pytorch.PyTorchTrialContext" title="determined.pytorch.PyTorchTrialContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrialContext</span></code></a>.
We support arbitrary numbers of models, optimizers, and LR schedulers
and arbitrary orders of running forward and backward passes.</p>
</li>
<li><p><strong>Configure automatic mixed precision</strong>.</p>
<p>In the <a class="reference internal" href="#determined.pytorch.PyTorchTrial.__init__" title="determined.pytorch.PyTorchTrial.__init__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code></a> method, call <code class="docutils literal notranslate"><span class="pre">configure_apex_amp</span></code> provided by
<a class="reference internal" href="#determined.pytorch.PyTorchTrialContext" title="determined.pytorch.PyTorchTrialContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrialContext</span></code></a>.</p>
</li>
<li><p><strong>Clip gradients</strong>.</p>
<p>In <a class="reference internal" href="#determined.pytorch.PyTorchTrial.train_batch" title="determined.pytorch.PyTorchTrial.train_batch"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_batch()</span></code></a>, pass a function into
<code class="docutils literal notranslate"><span class="pre">step_optimizer(optimizer,</span> <span class="pre">clip_grads=...)</span></code> provided by
<a class="reference internal" href="#determined.pytorch.PyTorchTrialContext" title="determined.pytorch.PyTorchTrialContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrialContext</span></code></a>.</p>
</li>
</ul>
<dl class="py attribute">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrial.trial_context_class">
<span class="sig-name descname"><span class="pre">trial_context_class</span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrial.trial_context_class" title="Permalink to this definition">#</a></dt>
<dd><p>alias of <a class="reference internal" href="#determined.pytorch.PyTorchTrialContext" title="determined.pytorch._pytorch_context.PyTorchTrialContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTrialContext</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrial.__init__">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#determined.pytorch.PyTorchTrialContext" title="determined.pytorch._pytorch_context.PyTorchTrialContext"><span class="pre">PyTorchTrialContext</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrial.__init__" title="Permalink to this definition">#</a></dt>
<dd><p>Initializes a trial using the provided <code class="docutils literal notranslate"><span class="pre">context</span></code>. The general steps are:</p>
<ol class="arabic simple">
<li><p>Initialize model(s) and wrap them with <code class="docutils literal notranslate"><span class="pre">context.wrap_model</span></code>.</p></li>
<li><p>Initialize optimizer(s) and wrap them with <code class="docutils literal notranslate"><span class="pre">context.wrap_optimizer</span></code>.</p></li>
<li><p>Initialize learning rate schedulers and wrap them with <code class="docutils literal notranslate"><span class="pre">context.wrap_lr_scheduler</span></code>.</p></li>
<li><p>If desired, wrap models and optimizer with <code class="docutils literal notranslate"><span class="pre">context.configure_apex_amp</span></code>
to use <code class="docutils literal notranslate"><span class="pre">apex.amp</span></code> for automatic mixed precision.</p></li>
<li><p>Define custom loss function and metric functions.</p></li>
</ol>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>You may see metrics for trials that are paused and later continued that are significantly
different from trials that are not paused if some of your models, optimizers, and
learning rate schedulers are not wrapped. The reason is that the model’s state may not be
restored accurately or completely from the checkpoint, which is saved to a checkpoint and
then later loaded into the trial during resumed training. When using PyTorch, this can
sometimes happen if the PyTorch API is not used correctly.</p>
</div>
<p>Here is a code example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">context</span>

<span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">wrap_model</span><span class="p">(</span><span class="n">MyModelA</span><span class="p">())</span>
<span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">wrap_model</span><span class="p">(</span><span class="n">MyModelB</span><span class="p">())</span>
<span class="bp">self</span><span class="o">.</span><span class="n">opt1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">wrap_optimizer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optm</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">))</span>
<span class="bp">self</span><span class="o">.</span><span class="n">opt2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">wrap_optimizer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optm</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">))</span>

<span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">),</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">opt1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt2</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">configure_apex_amp</span><span class="p">(</span>
    <span class="n">models</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">],</span>
    <span class="n">optimizers</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">opt1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt2</span><span class="p">],</span>
    <span class="n">num_losses</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>

<span class="bp">self</span><span class="o">.</span><span class="n">lrs1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">wrap_lr_scheduler</span><span class="p">(</span>
    <span class="n">lr_scheduler</span><span class="o">=</span><span class="n">LambdaLR</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">opt1</span><span class="p">,</span> <span class="n">lr_lambda</span><span class="o">=</span><span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="mf">0.95</span> <span class="o">**</span> <span class="n">epoch</span><span class="p">),</span>
    <span class="n">step_mode</span><span class="o">=</span><span class="n">LRScheduler</span><span class="o">.</span><span class="n">StepMode</span><span class="o">.</span><span class="n">STEP_EVERY_EPOCH</span><span class="p">,</span>
<span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrial.train_batch">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">train_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrial.train_batch" title="Permalink to this definition">#</a></dt>
<dd><p>Train on one batch.</p>
<p>Users should implement this function by doing the following things:</p>
<ol class="arabic simple">
<li><p>Run forward passes on the models.</p></li>
<li><p>Calculate the gradients with the losses with <code class="docutils literal notranslate"><span class="pre">context.backward</span></code>.</p></li>
<li><p>Call an optimization step for the optimizers with <code class="docutils literal notranslate"><span class="pre">context.step_optimizer</span></code>.
You can clip gradients by specifying the argument <code class="docutils literal notranslate"><span class="pre">clip_grads</span></code>.</p></li>
<li><p>Step LR schedulers if using manual step mode.</p></li>
<li><p>Return training metrics in a dictionary.</p></li>
</ol>
<p>Here is a code example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assume two models, two optimizers, and two LR schedulers were initialized</span>
<span class="c1"># in ``__init__``.</span>

<span class="c1"># Calculate the losses using the models.</span>
<span class="n">loss1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model1</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">loss2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model2</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

<span class="c1"># Run backward passes on losses and step optimizers. These can happen</span>
<span class="c1"># in arbitrary orders.</span>
<span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss1</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss2</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">step_optimizer</span><span class="p">(</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">opt1</span><span class="p">,</span>
    <span class="n">clip_grads</span><span class="o">=</span><span class="k">lambda</span> <span class="n">params</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">),</span>
<span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">step_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">opt2</span><span class="p">)</span>

<span class="c1"># Step the learning rate.</span>
<span class="bp">self</span><span class="o">.</span><span class="n">lrs1</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="bp">self</span><span class="o">.</span><span class="n">lrs2</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="k">return</span> <span class="p">{</span><span class="s2">&quot;loss1&quot;</span><span class="p">:</span> <span class="n">loss1</span><span class="p">,</span> <span class="s2">&quot;loss2&quot;</span><span class="p">:</span> <span class="n">loss2</span><span class="p">}</span>
</pre></div>
</div>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>batch (Dict[str, torch.Tensor], Sequence[torch.Tensor], torch.Tensor):</dt><dd><p>batch of data for training.</p>
</dd>
<dt>epoch_idx (integer): index of the current epoch among all the batches processed</dt><dd><p>per device (slot) since the start of training.</p>
</dd>
<dt>batch_idx (integer): index of the current batch among all the epochs processed</dt><dd><p>per device (slot) since the start of training.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="simple">
<dt>torch.Tensor or Dict[str, Any]:</dt><dd><p>training metrics to return.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrial.build_training_data_loader">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">build_training_data_loader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#determined.pytorch.DataLoader" title="determined.pytorch._data.DataLoader"><span class="pre">DataLoader</span></a></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrial.build_training_data_loader" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the data loader to use during training.</p>
<p>Must return an instance of <a class="reference internal" href="#determined.pytorch.DataLoader" title="determined.pytorch.DataLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">determined.pytorch.DataLoader</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrial.build_validation_data_loader">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">build_validation_data_loader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#determined.pytorch.DataLoader" title="determined.pytorch._data.DataLoader"><span class="pre">DataLoader</span></a></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrial.build_validation_data_loader" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the data loader to use during validation.</p>
<p>Must return an instance of <a class="reference internal" href="#determined.pytorch.DataLoader" title="determined.pytorch.DataLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">determined.pytorch.DataLoader</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrial.build_callbacks">
<span class="sig-name descname"><span class="pre">build_callbacks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#determined.pytorch.PyTorchCallback" title="determined.pytorch._callback.PyTorchCallback"><span class="pre">PyTorchCallback</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrial.build_callbacks" title="Permalink to this definition">#</a></dt>
<dd><p>Defines a dictionary of string names to callbacks to be used during
training and/or validation.</p>
<p>The string name will be used as the key to save and restore callback
state for any callback that defines <code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrial.evaluate_batch">
<span class="sig-name descname"><span class="pre">evaluate_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrial.evaluate_batch" title="Permalink to this definition">#</a></dt>
<dd><p>Calculate validation metrics for a batch and return them as a
dictionary mapping metric names to metric values. Per-batch validation metrics
are reduced (aggregated) to produce a single set of validation metrics for the
entire validation set (see <a class="reference internal" href="#determined.pytorch.PyTorchTrial.evaluation_reducer" title="determined.pytorch.PyTorchTrial.evaluation_reducer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluation_reducer()</span></code></a>).</p>
<p>There are two ways to specify evaluation metrics. Either override
<a class="reference internal" href="#determined.pytorch.PyTorchTrial.evaluate_batch" title="determined.pytorch.PyTorchTrial.evaluate_batch"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_batch()</span></code></a> or <a class="reference internal" href="#determined.pytorch.PyTorchTrial.evaluate_full_dataset" title="determined.pytorch.PyTorchTrial.evaluate_full_dataset"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_full_dataset()</span></code></a>. While
<a class="reference internal" href="#determined.pytorch.PyTorchTrial.evaluate_full_dataset" title="determined.pytorch.PyTorchTrial.evaluate_full_dataset"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_full_dataset()</span></code></a> is more flexible,
<a class="reference internal" href="#determined.pytorch.PyTorchTrial.evaluate_batch" title="determined.pytorch.PyTorchTrial.evaluate_batch"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_batch()</span></code></a> should be preferred, since it can be
parallelized in distributed environments, whereas
<a class="reference internal" href="#determined.pytorch.PyTorchTrial.evaluate_full_dataset" title="determined.pytorch.PyTorchTrial.evaluate_full_dataset"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_full_dataset()</span></code></a> cannot. Only one of
<a class="reference internal" href="#determined.pytorch.PyTorchTrial.evaluate_full_dataset" title="determined.pytorch.PyTorchTrial.evaluate_full_dataset"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_full_dataset()</span></code></a> and <a class="reference internal" href="#determined.pytorch.PyTorchTrial.evaluate_batch" title="determined.pytorch.PyTorchTrial.evaluate_batch"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_batch()</span></code></a> should be
overridden by a trial.</p>
<p>The metrics returned from this function must be JSON-serializable.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>batch (Dict[str, torch.Tensor], Sequence[torch.Tensor], torch.Tensor):</dt><dd><p>batch of data for evaluating.</p>
</dd>
<dt>batch_idx (integer): index of the current batch among all the epochs processed</dt><dd><p>per device (slot) since the start of training.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrial.evaluation_reducer">
<span class="sig-name descname"><span class="pre">evaluation_reducer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#determined.pytorch.Reducer" title="determined.pytorch._reducer.Reducer"><span class="pre">Reducer</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#determined.pytorch.Reducer" title="determined.pytorch._reducer.Reducer"><span class="pre">Reducer</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrial.evaluation_reducer" title="Permalink to this definition">#</a></dt>
<dd><p>Return a reducer for all evaluation metrics, or a dict mapping metric
names to individual reducers. Defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">determined.pytorch.Reducer.AVG</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrial.evaluate_full_dataset">
<span class="sig-name descname"><span class="pre">evaluate_full_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_loader</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">DataLoader</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrial.evaluate_full_dataset" title="Permalink to this definition">#</a></dt>
<dd><p>Calculate validation metrics on the entire validation dataset and
return them as a dictionary mapping metric names to reduced metric
values (i.e., each returned metric is the average or sum of that metric
across the entire validation set).</p>
<p>This validation cannot be distributed and is performed on a single
device, even when multiple devices (slots) are used for training. Only
one of <a class="reference internal" href="#determined.pytorch.PyTorchTrial.evaluate_full_dataset" title="determined.pytorch.PyTorchTrial.evaluate_full_dataset"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_full_dataset()</span></code></a> and <a class="reference internal" href="#determined.pytorch.PyTorchTrial.evaluate_batch" title="determined.pytorch.PyTorchTrial.evaluate_batch"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_batch()</span></code></a> should
be overridden by a trial.</p>
<p>The metrics returned from this function must be JSON-serializable.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>data_loader (torch.utils.data.DataLoader): data loader for evaluating.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrial.get_batch_length">
<span class="sig-name descname"><span class="pre">get_batch_length</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrial.get_batch_length" title="Permalink to this definition">#</a></dt>
<dd><p>Count the number of records in a given batch.</p>
<p>Override this method when you are using custom batch types, as produced
when iterating over the <a class="reference internal" href="#determined.pytorch.DataLoader" title="determined.pytorch.DataLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">determined.pytorch.DataLoader</span></code></a>.
For example, when using <code class="docutils literal notranslate"><span class="pre">pytorch_geometric</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Extra imports:</span>
<span class="kn">from</span> <span class="nn">determined.pytorch</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch_geometric.data.dataloader</span> <span class="kn">import</span> <span class="n">Collater</span>

<span class="c1"># Trial methods:</span>
<span class="k">def</span> <span class="nf">build_training_data_loader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_subset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">get_per_slot_batch_size</span><span class="p">(),</span>
        <span class="n">collate_fn</span><span class="o">=</span><span class="n">Collater</span><span class="p">([],</span> <span class="p">[]),</span>
    <span class="p">)</span>

<span class="k">def</span> <span class="nf">get_batch_length</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="c1"># `batch` is `torch_geometric.data.batch.Batch`.</span>
    <span class="k">return</span> <span class="n">batch</span><span class="o">.</span><span class="n">num_graphs</span>
</pre></div>
</div>
<dl class="simple">
<dt>Arguments:</dt><dd><p>batch (Any): input training or validation data batch object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="determined-pytorch-pytorchtrialcontext">
<h2><code class="docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchTrialContext</span></code><a class="headerlink" href="#determined-pytorch-pytorchtrialcontext" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">determined.pytorch.</span></span><span class="sig-name descname"><span class="pre">PyTorchTrialContext</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">core_context</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="api-core-reference.html#determined.core.Context" title="determined.core._context.Context"><span class="pre">Context</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">trial_seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">slots_per_trial</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_gpus</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_conf</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aggregation_frequency</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">steps_completed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">managed_training</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">debug_enabled</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext" title="Permalink to this definition">#</a></dt>
<dd><p>Contains runtime information for any Determined workflow that uses the <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> API.</p>
<p>With this class, users can do the following things:</p>
<ol class="arabic simple">
<li><p>Wrap PyTorch models, optimizers, and LR schedulers with their Determined-compatible
counterparts using <a class="reference internal" href="#determined.pytorch.PyTorchTrialContext.wrap_model" title="determined.pytorch.PyTorchTrialContext.wrap_model"><code class="xref py py-meth docutils literal notranslate"><span class="pre">wrap_model()</span></code></a>, <a class="reference internal" href="#determined.pytorch.PyTorchTrialContext.wrap_optimizer" title="determined.pytorch.PyTorchTrialContext.wrap_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">wrap_optimizer()</span></code></a>, <a class="reference internal" href="#determined.pytorch.PyTorchTrialContext.wrap_lr_scheduler" title="determined.pytorch.PyTorchTrialContext.wrap_lr_scheduler"><code class="xref py py-meth docutils literal notranslate"><span class="pre">wrap_lr_scheduler()</span></code></a>,
respectively. The Determined-compatible objects are capable of transparent
distributed training, checkpointing and exporting, mixed-precision training,
and gradient aggregation.</p></li>
<li><p>Configure apex amp by calling <a class="reference internal" href="#determined.pytorch.PyTorchTrialContext.configure_apex_amp" title="determined.pytorch.PyTorchTrialContext.configure_apex_amp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_apex_amp()</span></code></a> (optional).</p></li>
<li><p>Calculate the gradients with <a class="reference internal" href="#determined.pytorch.PyTorchTrialContext.backward" title="determined.pytorch.PyTorchTrialContext.backward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></a> on a specified loss.</p></li>
<li><p>Run an optimization step with <a class="reference internal" href="#determined.pytorch.PyTorchTrialContext.step_optimizer" title="determined.pytorch.PyTorchTrialContext.step_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">step_optimizer()</span></code></a>.</p></li>
<li><p>Functionalities inherited from <code class="xref py py-class docutils literal notranslate"><span class="pre">determined.TrialContext</span></code>, including getting
the runtime information and properly handling training data in distributed training.</p></li>
</ol>
<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retain_graph</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">create_graph</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.backward" title="Permalink to this definition">#</a></dt>
<dd><p>Compute the gradient of current tensor w.r.t. graph leaves.</p>
<p>The arguments are used in the same way as <code class="docutils literal notranslate"><span class="pre">torch.Tensor.backward</span></code>.
See <a class="reference external" href="https://pytorch.org/docs/1.4.0/_modules/torch/tensor.html#Tensor.backward">https://pytorch.org/docs/1.4.0/_modules/torch/tensor.html#Tensor.backward</a> for details.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When using distributed training, we don’t support manual gradient accumulation.
That means the gradient on each parameter can only be calculated once on each batch.
If a parameter is associated with multiple losses, you can either choose to call
<code class="docutils literal notranslate"><span class="pre">backward''</span> <span class="pre">on</span> <span class="pre">only</span> <span class="pre">one</span> <span class="pre">of</span> <span class="pre">those</span> <span class="pre">losses,</span> <span class="pre">or</span> <span class="pre">you</span> <span class="pre">can</span> <span class="pre">set</span> <span class="pre">the</span> <span class="pre">``require_grads</span></code> flag of
a parameter or module to <code class="docutils literal notranslate"><span class="pre">False</span></code> to avoid manual gradient accumulation on that
parameter.
However, you can do gradient accumulation across batches by setting
<a class="reference internal" href="experiment-config-reference.html#config-aggregation-frequency"><span class="std std-ref">optimizations.aggregation_frequency</span></a> in the
experiment configuration to be greater than 1.</p>
</div>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>gradient (Tensor or None): Gradient w.r.t. the</dt><dd><p>tensor. If it is a tensor, it will be automatically converted
to a Tensor that does not require grad unless <code class="docutils literal notranslate"><span class="pre">create_graph</span></code> is True.
None values can be specified for scalar Tensors or ones that
don’t require grad. If a None value would be acceptable then
this argument is optional.</p>
</dd>
<dt>retain_graph (bool, optional): If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the graph used to compute</dt><dd><p>the grads will be freed. Note that in nearly all cases setting
this option to True is not needed and often can be worked around
in a much more efficient way. Defaults to the value of
<code class="docutils literal notranslate"><span class="pre">create_graph</span></code>.</p>
</dd>
<dt>create_graph (bool, optional): If <code class="docutils literal notranslate"><span class="pre">True</span></code>, graph of the derivative will</dt><dd><p>be constructed, allowing to compute higher order derivative
products. Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.configure_apex_amp">
<span class="sig-name descname"><span class="pre">configure_apex_amp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">models</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Optimizer</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enabled</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt_level</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'O1'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cast_model_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dtype</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patch_torch_functions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_batchnorm_fp32</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">master_weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cast_model_outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dtype</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_losses</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbosity</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_loss_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_loss_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">16777216.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.configure_apex_amp" title="Permalink to this definition">#</a></dt>
<dd><p>Configure automatic mixed precision for your models and optimizers using NVIDIA’s Apex
PyTorch extension. Note that details for <code class="docutils literal notranslate"><span class="pre">apex.amp</span></code> are handled automatically within
Determined after this call.</p>
<p>This function must be called <strong>after</strong> you have finished constructing your models and
optimizers with <a class="reference internal" href="#determined.pytorch.PyTorchTrialContext.wrap_model" title="determined.pytorch.PyTorchTrialContext.wrap_model"><code class="xref py py-meth docutils literal notranslate"><span class="pre">wrap_model()</span></code></a> and <a class="reference internal" href="#determined.pytorch.PyTorchTrialContext.wrap_optimizer" title="determined.pytorch.PyTorchTrialContext.wrap_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">wrap_optimizer()</span></code></a>.</p>
<p>This function has the same arguments as
<a class="reference external" href="https://nvidia.github.io/apex/amp.html#apex.amp.initialize">apex.amp.initialize</a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When using distributed training and automatic mixed precision,
we only support <code class="docutils literal notranslate"><span class="pre">num_losses=1</span></code> and calling backward on the loss once.</p>
</div>
<dl>
<dt>Arguments:</dt><dd><p>models (<code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> or list of <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> s):  Model(s) to modify/cast.
optimizers (<code class="docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code> or list of <code class="docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code> s):</p>
<blockquote>
<div><p>Optimizers to modify/cast. REQUIRED for training.</p>
</div></blockquote>
<dl class="simple">
<dt>enabled (bool, optional, default=True):  If False, renders all Amp calls no-ops,</dt><dd><p>so your script should run as if Amp were not present.</p>
</dd>
<dt>opt_level (str, optional, default=”O1”):  Pure or mixed precision optimization level.</dt><dd><p>Accepted values are “O0”, “O1”, “O2”, and “O3”, explained in detail above.</p>
</dd>
<dt>cast_model_type (<code class="docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional, default=None):  Optional property override,</dt><dd><p>see above.</p>
</dd>
</dl>
<p>patch_torch_functions (bool, optional, default=None):  Optional property override.
keep_batchnorm_fp32 (bool or str, optional, default=None):  Optional property override.</p>
<blockquote>
<div><p>If passed as a string, must be the string “True” or “False”.</p>
</div></blockquote>
<p>master_weights (bool, optional, default=None):  Optional property override.
loss_scale (float or str, optional, default=None):  Optional property override.</p>
<blockquote>
<div><p>If passed as a string, must be a string representing a number, e.g., “128.0”,
or the string “dynamic”.</p>
</div></blockquote>
<dl class="simple">
<dt>cast_model_outputs (torch.dtype, optional, default=None):  Option to ensure that</dt><dd><p>the outputs of your model is always cast to a particular type regardless of
<code class="docutils literal notranslate"><span class="pre">opt_level</span></code>.</p>
</dd>
<dt>num_losses (int, optional, default=1):  Option to tell Amp in advance how many</dt><dd><p>losses/backward passes you plan to use.  When used in conjunction with the
<code class="docutils literal notranslate"><span class="pre">loss_id</span></code> argument to <code class="docutils literal notranslate"><span class="pre">amp.scale_loss</span></code>, enables Amp to use a different
loss scale per loss/backward pass, which can improve stability.
If <code class="docutils literal notranslate"><span class="pre">num_losses</span></code> is left to 1, Amp will still support multiple losses/backward
passes, but use a single global loss scale for all of them.</p>
</dd>
</dl>
<p>verbosity (int, default=1):  Set to 0 to suppress Amp-related output.
min_loss_scale (float, default=None):  Sets a floor for the loss scale values that</p>
<blockquote>
<div><p>can be chosen by dynamic loss scaling.  The default value of None means that no
floor is imposed. If dynamic loss scaling is not used, <code class="docutils literal notranslate"><span class="pre">min_loss_scale</span></code> is
ignored.</p>
</div></blockquote>
<dl class="simple">
<dt>max_loss_scale (float, default=2.**24):  Sets a ceiling for the loss scale values</dt><dd><p>that can be chosen by dynamic loss scaling.  If dynamic loss scaling is not used,
<code class="docutils literal notranslate"><span class="pre">max_loss_scale</span></code> is ignored.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Model(s) and optimizer(s) modified according to the <code class="docutils literal notranslate"><span class="pre">opt_level</span></code>.
If  <code class="docutils literal notranslate"><span class="pre">optimizers</span></code> args were lists, the corresponding return value will
also be a list.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.current_train_batch">
<span class="sig-name descname"><span class="pre">current_train_batch</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.current_train_batch" title="Permalink to this definition">#</a></dt>
<dd><p>Current global batch index</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.get_data_config">
<span class="sig-name descname"><span class="pre">get_data_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.get_data_config" title="Permalink to this definition">#</a></dt>
<dd><p>Return the data configuration.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.get_experiment_id">
<span class="sig-name descname"><span class="pre">get_experiment_id</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.get_experiment_id" title="Permalink to this definition">#</a></dt>
<dd><p>Return the experiment ID of the current trial.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.get_global_batch_size">
<span class="sig-name descname"><span class="pre">get_global_batch_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.get_global_batch_size" title="Permalink to this definition">#</a></dt>
<dd><p>Return the global batch size.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.get_hparam">
<span class="sig-name descname"><span class="pre">get_hparam</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.get_hparam" title="Permalink to this definition">#</a></dt>
<dd><p>Return the current value of the hyperparameter with the given name.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.get_per_slot_batch_size">
<span class="sig-name descname"><span class="pre">get_per_slot_batch_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.get_per_slot_batch_size" title="Permalink to this definition">#</a></dt>
<dd><p>Return the per-slot batch size. When a model is trained with a single GPU, this is equal to
the global batch size. When multi-GPU training is used, this is equal to the global batch
size divided by the number of GPUs used to train the model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.get_stop_requested">
<span class="sig-name descname"><span class="pre">get_stop_requested</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.get_stop_requested" title="Permalink to this definition">#</a></dt>
<dd><p>Return whether a trial stoppage has been requested.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.get_tensorboard_path">
<span class="sig-name descname"><span class="pre">get_tensorboard_path</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Path</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.get_tensorboard_path" title="Permalink to this definition">#</a></dt>
<dd><p>Get the path where files for consumption by TensorBoard should be written</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.get_tensorboard_writer">
<span class="sig-name descname"><span class="pre">get_tensorboard_writer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.get_tensorboard_writer" title="Permalink to this definition">#</a></dt>
<dd><p>This function returns an instance of <code class="docutils literal notranslate"><span class="pre">torch.utils.tensorboard.SummaryWriter</span></code></p>
<p>Trials users who wish to log to TensorBoard can use this writer object.
We provide and manage a writer in order to save and upload TensorBoard
files automatically on behalf of the user.</p>
<p>Usage example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">PyTorchTrial</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">writer</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_tensorboard_writer</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">train_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">epoch_idx</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;my_metric&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(),</span> <span class="n">batch_idx</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;my_image&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">)),</span> <span class="n">batch_idx</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.get_trial_id">
<span class="sig-name descname"><span class="pre">get_trial_id</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.get_trial_id" title="Permalink to this definition">#</a></dt>
<dd><p>Return the trial ID of the current trial.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.is_epoch_end">
<span class="sig-name descname"><span class="pre">is_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.is_epoch_end" title="Permalink to this definition">#</a></dt>
<dd><p>Returns true if the current batch is the last batch of the epoch.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Not accurate for variable size epochs.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.is_epoch_start">
<span class="sig-name descname"><span class="pre">is_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.is_epoch_start" title="Permalink to this definition">#</a></dt>
<dd><p>Returns true if the current batch is the first batch of the epoch.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Not accurate for variable size epochs.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.set_profiler">
<span class="sig-name descname"><span class="pre">set_profiler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.set_profiler" title="Permalink to this definition">#</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">set_profiler()</span></code> is a thin wrapper around the native PyTorch profiler, torch-tb-profiler.
It overrides the <code class="docutils literal notranslate"><span class="pre">on_trace_ready</span></code> parameter to the determined tensorboard path, while all
other arguments are passed directly into <code class="docutils literal notranslate"><span class="pre">torch.profiler.profile</span></code>. Stepping the profiler
will be handled automatically during the training loop.</p>
<p>See the <a class="reference external" href="https://github.com/pytorch/kineto/tree/master/tb_plugin">PyTorch profiler plugin</a> for details.</p>
<p>Examples:</p>
<p>Profiling GPU and CPU activities, skipping batch 1, warming up on batch 2, and profiling
batches 3 and 4.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">set_profiler</span><span class="p">(</span>
    <span class="n">activities</span><span class="o">=</span><span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">schedule</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">schedule</span><span class="p">(</span>
        <span class="n">wait</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">warmup</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">active</span><span class="o">=</span><span class="mi">2</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.set_stop_requested">
<span class="sig-name descname"><span class="pre">set_stop_requested</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stop_requested</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.set_stop_requested" title="Permalink to this definition">#</a></dt>
<dd><p>Set a flag to request a trial stoppage. When this flag is set to True,
we finish the step, checkpoint, then exit.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.step_optimizer">
<span class="sig-name descname"><span class="pre">step_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_grads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Iterator</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">auto_zero_grads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.step_optimizer" title="Permalink to this definition">#</a></dt>
<dd><p>Perform a single optimization step.</p>
<p>This function must be called once for each optimizer. However, the order of
different optimizers’ steps can be specified by calling this function in different
orders. Also, gradient accumulation across iterations is performed by the Determined
training loop by setting the experiment configuration field
<a class="reference internal" href="experiment-config-reference.html#config-aggregation-frequency"><span class="std std-ref">optimizations.aggregation_frequency</span></a>.</p>
<p>Here is a code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">clip_grads</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">),</span>

<span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">step_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">opt1</span><span class="p">,</span> <span class="n">clip_grads</span><span class="p">)</span>
</pre></div>
</div>
<dl>
<dt>Arguments:</dt><dd><p>optimizer(<code class="docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code>): Which optimizer should be stepped.
clip_grads(a function, optional): This function should have one argument for</p>
<blockquote>
<div><p>parameters in order to clip the gradients.</p>
</div></blockquote>
<dl class="simple">
<dt>auto_zero_grads(bool, optional): Automatically zero out gradients automatically after</dt><dd><p>stepping the optimizer. If false, you need to call <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>
manually. Note that if <a class="reference internal" href="experiment-config-reference.html#config-aggregation-frequency"><span class="std std-ref">optimizations.aggregation_frequency</span></a> is greater than 1, <code class="docutils literal notranslate"><span class="pre">auto_zero_grads</span></code> must be true.</p>
</dd>
<dt>scaler(<code class="docutils literal notranslate"><span class="pre">torch.cuda.amp.GradScaler</span></code>, optional): The scaler to use for stepping the</dt><dd><p>optimizer. This should be unset if not using AMP, and is necessary if
<code class="docutils literal notranslate"><span class="pre">wrap_scaler()</span></code> was called directly.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.to_device">
<span class="sig-name descname"><span class="pre">to_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.to_device" title="Permalink to this definition">#</a></dt>
<dd><p>Map generated data to the device allocated by the Determined cluster.</p>
<p>All the data in the data loader and the models are automatically moved to the
allocated device. This method aims at providing a function for the data generated
on the fly.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.wrap_lr_scheduler">
<span class="sig-name descname"><span class="pre">wrap_lr_scheduler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lr_scheduler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_LRScheduler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#determined.pytorch.LRScheduler.StepMode" title="determined.pytorch._lr_scheduler.LRScheduler.StepMode"><span class="pre">StepMode</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">frequency</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_LRScheduler</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.wrap_lr_scheduler" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a wrapped LR scheduler.</p>
<p>The LR scheduler must use an optimizer wrapped by <a class="reference internal" href="#determined.pytorch.PyTorchTrialContext.wrap_optimizer" title="determined.pytorch.PyTorchTrialContext.wrap_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">wrap_optimizer()</span></code></a>.  If <code class="docutils literal notranslate"><span class="pre">apex.amp</span></code>
is in use, the optimizer must also have been configured with <a class="reference internal" href="#determined.pytorch.PyTorchTrialContext.configure_apex_amp" title="determined.pytorch.PyTorchTrialContext.configure_apex_amp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_apex_amp()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.wrap_model">
<span class="sig-name descname"><span class="pre">wrap_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Module</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.wrap_model" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a wrapped model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.wrap_optimizer">
<span class="sig-name descname"><span class="pre">wrap_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backward_passes_per_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fp16_compression</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average_aggregated_gradients</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Optimizer</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.wrap_optimizer" title="Permalink to this definition">#</a></dt>
<dd><p>Returns a wrapped optimizer.</p>
<p>The optimizer must use the models wrapped by <a class="reference internal" href="#determined.pytorch.PyTorchTrialContext.wrap_model" title="determined.pytorch.PyTorchTrialContext.wrap_model"><code class="xref py py-meth docutils literal notranslate"><span class="pre">wrap_model()</span></code></a>. This function
creates a <code class="docutils literal notranslate"><span class="pre">horovod.DistributedOptimizer</span></code> if using parallel/distributed training.</p>
<p><code class="docutils literal notranslate"><span class="pre">backward_passes_per_step</span></code> can be used to specify how many gradient aggregation
steps will be performed in a single <code class="docutils literal notranslate"><span class="pre">train_batch</span></code> call per optimizer step.
In most cases, this will just be the default value 1.  However, this advanced functionality
can be used to support training loops like the one shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_batch</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TorchData</span><span class="p">,</span> <span class="n">epoch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss1</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;loss1&#39;</span><span class="p">]</span>
    <span class="n">loss2</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;loss2&#39;</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss2</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">step_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">backward_passes_per_step</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;loss1&quot;</span><span class="p">:</span> <span class="n">loss1</span><span class="p">,</span> <span class="s2">&quot;loss2&quot;</span><span class="p">:</span> <span class="n">loss2</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchTrialContext.wrap_scaler">
<span class="sig-name descname"><span class="pre">wrap_scaler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scaler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchTrialContext.wrap_scaler" title="Permalink to this definition">#</a></dt>
<dd><p>Prepares to use automatic mixed precision through PyTorch’s native AMP API. The returned
scaler should be passed to <code class="docutils literal notranslate"><span class="pre">step_optimizer</span></code>, but usage does not otherwise differ from
vanilla PyTorch APIs. Loss should be scaled before calling <code class="docutils literal notranslate"><span class="pre">backward</span></code>, <code class="docutils literal notranslate"><span class="pre">unscale_</span></code> should
be called before clipping gradients, <code class="docutils literal notranslate"><span class="pre">update</span></code> should be called after stepping all
optimizers, etc.</p>
<p>PyTorch 1.6 or greater is required for this feature.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>scaler (<code class="docutils literal notranslate"><span class="pre">torch.cuda.amp.GradScaler</span></code>):  Scaler to wrap and track.</p>
</dd>
<dt>Returns:</dt><dd><p>The scaler. It may be wrapped to add additional functionality for use in Determined.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="determined-pytorch-pytorchtrialcontext-distributed">
<h2><code class="docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchTrialContext.distributed</span></code><a class="headerlink" href="#determined-pytorch-pytorchtrialcontext-distributed" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">determined.core._distributed.</span></span><span class="sig-name descname"><span class="pre">DistributedContext</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cross_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cross_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chief_ip</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pub_port</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">12360</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pull_port</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">12376</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">port_offset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_tcp</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>DistributedContext provides useful methods for effective distributed training.</p>
<dl class="simple">
<dt>A DistributedContext has the following required args:</dt><dd><ul class="simple">
<li><p>rank: the index of this worker in the entire job</p></li>
<li><p>size: the number of workers in the entire job</p></li>
<li><p>local_rank: the index of this worker on this machine</p></li>
<li><p>local_size: the number of workers on this machine</p></li>
<li><p>cross_rank: the index of this machine in the entire job</p></li>
<li><p>cross_size: the number of machines in the entire job</p></li>
</ul>
</dd>
<dt>Additionally, any time that cross_size &gt; 1, you must also provide:</dt><dd><ul class="simple">
<li><p>chief_ip: the ip address to reach the chief worker (where rank==0)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>DistributedContext has <code class="docutils literal notranslate"><span class="pre">.allgather()</span></code>, <code class="docutils literal notranslate"><span class="pre">.gather()</span></code>, and <code class="docutils literal notranslate"><span class="pre">.broadcast()</span></code> methods, which
are easy to use and which can be useful for coordinating work across workers, but it is not a
replacement for the allgather/gather/broadcast operations in your particular distributed
training framework.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_horovod</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hvd</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chief_ip</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="api-core-reference.html#determined.core.DistributedContext" title="determined.core._distributed.DistributedContext"><span class="pre">DistributedContext</span></a></span></span></dt>
<dd><p>Create a <code class="docutils literal notranslate"><span class="pre">DistributedContext</span></code> using the provided <code class="docutils literal notranslate"><span class="pre">hvd</span></code> module to determine rank
information.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">horovod.torch</span> <span class="k">as</span> <span class="nn">hvd</span>
<span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">distributed</span> <span class="o">=</span> <span class="n">DistributedContext</span><span class="o">.</span><span class="n">from_horovod</span><span class="p">(</span><span class="n">hvd</span><span class="p">)</span>
</pre></div>
</div>
<p>The IP address for the chief worker is required whenever <code class="docutils literal notranslate"><span class="pre">hvd.cross_size()</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>. The value
may be provided using the <code class="docutils literal notranslate"><span class="pre">chief_ip</span></code> argument or the <code class="docutils literal notranslate"><span class="pre">DET_CHIEF_IP</span></code> environment
variable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_deepspeed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">chief_ip</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="api-core-reference.html#determined.core.DistributedContext" title="determined.core._distributed.DistributedContext"><span class="pre">DistributedContext</span></a></span></span></dt>
<dd><p>Create a <code class="docutils literal notranslate"><span class="pre">DistributedContext</span></code> using the standard deepspeed environment variables to
determine rank information.</p>
<p>The IP address for the chief worker is required whenever CROSS_SIZE &gt; 1.  The value may
be provided using the chief_ip argument or the DET_CHIEF_IP environment variable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_torch_distributed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">chief_ip</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="api-core-reference.html#determined.core.DistributedContext" title="determined.core._distributed.DistributedContext"><span class="pre">DistributedContext</span></a></span></span></dt>
<dd><p>Create a DistributedContext using the standard torch distributed environment variables to
determine rank information.</p>
<p>The IP address for the chief worker is required whenever CROSS_SIZE &gt; 1.  The value may
be provided via the chief_ip argument or the DET_CHIEF_IP environment variable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">get_rank</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span></dt>
<dd><p>Return the rank of the process in the trial. The rank of a process is a
unique ID within the trial.  That is, no two processes in the same trial
are assigned the same rank.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">get_local_rank</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span></dt>
<dd><p>Return the rank of the process on the agent. The local rank of a process
is a unique ID within a given agent and trial; that is, no two processes
in the same trial that are executing on the same agent are assigned the
same rank.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">get_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span></dt>
<dd><p>Return the number of slots this trial is running on.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">get_num_agents</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span></dt>
<dd><p>Return the number of agents this trial is running on.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stuff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></span></dt>
<dd><p>Gather <code class="docutils literal notranslate"><span class="pre">stuff</span></code> to the chief.  The chief returns a list of all stuff, and workers return
<code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">gather()</span></code> is not a replacement for the gather functionality of your distributed training
framework.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">gather_local</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stuff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></span></dt>
<dd><p>Gather <code class="docutils literal notranslate"><span class="pre">stuff</span></code> to the local chief.  The local chief returns a list of all stuff, and local
workers return <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">gather_local()</span></code> is not a replacement for the gather functionality of your distributed
training framework.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">allgather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stuff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span></span></span></dt>
<dd><p>Gather <code class="docutils literal notranslate"><span class="pre">stuff</span></code> to the chief and broadcast all of it back to the workers.</p>
<p><code class="docutils literal notranslate"><span class="pre">allgather()</span></code> is not a replacement for the allgather functionality of your distributed
training framework.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">allgather_local</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stuff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span></span></span></dt>
<dd><p>Gather <code class="docutils literal notranslate"><span class="pre">stuff</span></code> to the local chief and broadcast all of it back to the local workers.</p>
<p><code class="docutils literal notranslate"><span class="pre">allgather_local()</span></code> is not a replacement for the allgather functionality of your
distributed training framework.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">broadcast</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stuff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span></dt>
<dd><p>Every worker gets the <code class="docutils literal notranslate"><span class="pre">stuff</span></code> sent by the chief.</p>
<p><code class="docutils literal notranslate"><span class="pre">broadcast()</span></code> is not a replacement for the broadcast functionality of your distributed
training framework.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">broadcast_local</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stuff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span></dt>
<dd><p>Every worker gets the <code class="docutils literal notranslate"><span class="pre">stuff</span></code> sent by the local chief.</p>
<p><code class="docutils literal notranslate"><span class="pre">broadcast_local()</span></code> is not a replacement for the broadcast functionality of your
distributed training framework.</p>
</dd></dl>

</dd></dl>

</section>
<section id="determined-pytorch-pytorchexperimentalcontext">
<h2><code class="docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchExperimentalContext</span></code><a class="headerlink" href="#determined-pytorch-pytorchexperimentalcontext" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchExperimentalContext">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">determined.pytorch.</span></span><span class="sig-name descname"><span class="pre">PyTorchExperimentalContext</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parent</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#determined.pytorch.PyTorchExperimentalContext" title="Permalink to this definition">#</a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchExperimentalContext.disable_auto_to_device">
<span class="sig-name descname"><span class="pre">disable_auto_to_device</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchExperimentalContext.disable_auto_to_device" title="Permalink to this definition">#</a></dt>
<dd><p>Prevent the PyTorchTrialController from automatically moving batched data to device.
Call this if you want to override the default behavior of moving all items of a list,
tuple, and/or dict to the GPU. Then, you can control how data is moved to the GPU directly
in the <code class="docutils literal notranslate"><span class="pre">train_batch</span></code> and <code class="docutils literal notranslate"><span class="pre">evaluate_batch</span></code> methods of your PyTorchTrial definition.
You should call context.to_device on primitive data types that you do want to move to GPU
as in the example below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorchTrial methods.</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="n">context</span><span class="p">):</span> <span class="c1"># PyTorchTrial init</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">disable_auto_to_device</span><span class="p">()</span>
    <span class="o">...</span>

<span class="k">def</span> <span class="nf">train_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;img&quot;</span><span class="p">:</span>
            <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;img&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;img&quot;</span><span class="p">])</span>
    <span class="o">...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchExperimentalContext.disable_dataset_reproducibility_checks">
<span class="sig-name descname"><span class="pre">disable_dataset_reproducibility_checks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchExperimentalContext.disable_dataset_reproducibility_checks" title="Permalink to this definition">#</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">disable_dataset_reproducibility_checks()</span></code> allows you to return an arbitrary
<code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> from <a class="reference internal" href="#determined.pytorch.PyTorchTrial.build_training_data_loader" title="determined.pytorch.PyTorchTrial.build_training_data_loader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">build_training_data_loader()</span></code></a> or
<a class="reference internal" href="#determined.pytorch.PyTorchTrial.build_validation_data_loader" title="determined.pytorch.PyTorchTrial.build_validation_data_loader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">build_validation_data_loader()</span></code></a>.</p>
<p>Normally you would be required to return a <code class="docutils literal notranslate"><span class="pre">det.pytorch.DataLoader</span></code> instead, which would
guarantee that an appropriate <code class="docutils literal notranslate"><span class="pre">Sampler</span></code> is used that ensures:</p>
<ul class="simple">
<li><p>When <code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code>, the shuffle is reproducible.</p></li>
<li><p>The dataset will start at the right location, even after pausing/continuing.</p></li>
<li><p>Proper sharding is used during distributed training.</p></li>
</ul>
<p>However, there may be cases where either reproducibility of the dataset is not needed or
where the nature of the dataset may cause the <code class="docutils literal notranslate"><span class="pre">det.pytorch.DataLoader</span></code> to be unsuitable.</p>
<p>In those cases, you may call <code class="docutils literal notranslate"><span class="pre">disable_dataset_reproducibility_checks()</span></code> and you will be
free to return any <code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> you like.  Dataset reproducibility will
still be possible, but it will be your responsibility.  If desired, you may find the
<code class="docutils literal notranslate"><span class="pre">Sampler</span></code> classes in <a class="reference internal" href="api-pytorch-samplers-reference.html#module-determined.pytorch.samplers" title="determined.pytorch.samplers"><code class="xref py py-mod docutils literal notranslate"><span class="pre">determined.pytorch.samplers</span></code></a> to be helpful.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchExperimentalContext.use_amp">
<span class="sig-name descname"><span class="pre">use_amp</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchExperimentalContext.use_amp" title="Permalink to this definition">#</a></dt>
<dd><p>Handles all operations for the most simple cases automatically with a default gradient
scaler. Specifically, wraps forward pass in an autocast context, scales loss before
backward pass, unscales before clipping gradients, uses scaler when stepping
optimizer(s), and updates scaler afterwards. Do not call <code class="docutils literal notranslate"><span class="pre">wrap_scaler</span></code> directly when
using this method.</p>
<p>PyTorch 1.6 or greater is required for this feature.</p>
</dd></dl>

</dd></dl>

</section>
<section id="determined-pytorch-dataloader">
<span id="pytorch-dataloader"></span><h2><code class="docutils literal notranslate"><span class="pre">determined.pytorch.DataLoader</span></code><a class="headerlink" href="#determined-pytorch-dataloader" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="determined.pytorch.DataLoader">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">determined.pytorch.</span></span><span class="sig-name descname"><span class="pre">DataLoader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sampler</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_sampler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">BatchSampler</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">collate_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">T</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_last</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">worker_init_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multiprocessing_context</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefetch_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">persistent_workers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#determined.pytorch.DataLoader" title="Permalink to this definition">#</a></dt>
<dd><p>DataLoader is meant to contain a user’s <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>, configuration for
sampling data in batches, and performance configuration like
multiprocessing.</p>
<p>The __init__ function determines the defaults in the same way as a
<code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> would, so the behavior should be familiar.
However, the <code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataloader</span></code> that is used for training and
validation is not created until <code class="docutils literal notranslate"><span class="pre">get_data_loader(...)</span></code> is called. This is
done so that Determined can ensure that sampling restarts from the right location
and distributed sampling is handled correctly.</p>
<p>Note that the arguments are from PyTorch.</p>
<dl>
<dt>Arguments:</dt><dd><p>dataset (Dataset): dataset from which to load the data.
batch_size (int, optional): how many samples per batch to load (default: <code class="docutils literal notranslate"><span class="pre">1</span></code>).
shuffle (bool, optional): set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to have the data reshuffled</p>
<blockquote>
<div><p>at every epoch (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>).</p>
</div></blockquote>
<dl class="simple">
<dt>sampler (Sampler, optional): defines the strategy to draw samples from</dt><dd><p>the dataset. If specified, <code class="xref py py-attr docutils literal notranslate"><span class="pre">shuffle</span></code> must be <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt>batch_sampler (Sampler, optional): like <code class="xref py py-attr docutils literal notranslate"><span class="pre">sampler</span></code>, but returns a batch of</dt><dd><p>indices at a time. Mutually exclusive with <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch_size</span></code>,
<code class="xref py py-attr docutils literal notranslate"><span class="pre">shuffle</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">sampler</span></code>, and <code class="xref py py-attr docutils literal notranslate"><span class="pre">drop_last</span></code>.</p>
</dd>
<dt>num_workers (int, optional): how many subprocesses to use for data</dt><dd><p>loading. <code class="docutils literal notranslate"><span class="pre">0</span></code> means that the data will be loaded in the main process.
(default: <code class="docutils literal notranslate"><span class="pre">0</span></code>)</p>
</dd>
<dt>collate_fn (callable, optional): merges a list of samples to form a</dt><dd><p>mini-batch of Tensor(s).  Used when using batched loading from a
map-style dataset.</p>
</dd>
<dt>pin_memory (bool, optional): If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the data loader will copy Tensors</dt><dd><p>into CUDA pinned memory before returning them.  If your data elements
are a custom type, or your <code class="xref py py-attr docutils literal notranslate"><span class="pre">collate_fn</span></code> returns a batch that is a custom type,
see the example below.</p>
</dd>
<dt>drop_last (bool, optional): set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to drop the last incomplete batch,</dt><dd><p>if the dataset size is not divisible by the batch size. If <code class="docutils literal notranslate"><span class="pre">False</span></code> and
the size of dataset is not divisible by the batch size, then the last batch
will be smaller. (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p>
</dd>
<dt>timeout (numeric, optional): if positive, the timeout value for collecting a batch</dt><dd><p>from workers. Should always be non-negative. (default: <code class="docutils literal notranslate"><span class="pre">0</span></code>)</p>
</dd>
<dt>worker_init_fn (callable, optional): If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, this will be called on each</dt><dd><p>worker subprocess with the worker id (an int in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">num_workers</span> <span class="pre">-</span> <span class="pre">1]</span></code>) as
input, after seeding and before data loading. (default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p>
</dd>
<dt>generator (torch.Generator, optional): If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, this RNG will be used</dt><dd><p>by RandomSampler to generate random indexes and multiprocessing to generate
<code class="docutils literal notranslate"><span class="pre">base_seed</span></code> for workers. (default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p>
</dd>
<dt>prefetch_factor (int, optional, keyword-only arg): Number of samples loaded</dt><dd><p>in advance by each worker. <code class="docutils literal notranslate"><span class="pre">2</span></code> means there will be a total of
2 * num_workers samples prefetched across all workers. (default: <code class="docutils literal notranslate"><span class="pre">2</span></code>)</p>
</dd>
<dt>persistent_workers (bool, optional): If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the data loader will not shut down</dt><dd><p>the worker processes after a dataset has been consumed once. This allows to
maintain the workers <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> instances alive. (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</section>
<section id="determined-pytorch-lrscheduler">
<h2><code class="docutils literal notranslate"><span class="pre">determined.pytorch.LRScheduler</span></code><a class="headerlink" href="#determined-pytorch-lrscheduler" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="determined.pytorch.LRScheduler">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">determined.pytorch.</span></span><span class="sig-name descname"><span class="pre">LRScheduler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scheduler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_LRScheduler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#determined.pytorch.LRScheduler.StepMode" title="determined.pytorch._lr_scheduler.LRScheduler.StepMode"><span class="pre">StepMode</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">frequency</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#determined.pytorch.LRScheduler" title="Permalink to this definition">#</a></dt>
<dd><p>Wrapper for a PyTorch LRScheduler.</p>
<p>This wrapper fulfills two main functions:</p>
<ol class="arabic simple">
<li><p>Save and restore the learning rate when a trial is paused, preempted, etc.</p></li>
<li><p>Step the learning rate scheduler at the configured frequency
(e.g., every batch or every epoch).</p></li>
</ol>
<dl class="py class">
<dt class="sig sig-object py" id="determined.pytorch.LRScheduler.StepMode">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">StepMode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#determined.pytorch.LRScheduler.StepMode" title="Permalink to this definition">#</a></dt>
<dd><p>Specifies when and how scheduler.step() should be executed.</p>
<dl class="simple">
<dt>Attributes:</dt><dd><p>STEP_EVERY_EPOCH
STEP_EVERY_BATCH
MANUAL_STEP
STEP_EVERY_OPTIMIZER_STEP</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.LRScheduler.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scheduler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_LRScheduler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#determined.pytorch.LRScheduler.StepMode" title="determined.pytorch._lr_scheduler.LRScheduler.StepMode"><span class="pre">StepMode</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">frequency</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#determined.pytorch.LRScheduler.__init__" title="Permalink to this definition">#</a></dt>
<dd><p>LRScheduler constructor.</p>
<dl>
<dt>Args:</dt><dd><dl>
<dt>scheduler (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler._LRScheduler</span></code>):</dt><dd><p>Learning rate scheduler to be used by Determined.</p>
</dd>
<dt>step_mode (<code class="xref py py-class docutils literal notranslate"><span class="pre">determined.pytorch.LRSchedulerStepMode</span></code>):</dt><dd><p>The strategy Determined will use to call (or not call) scheduler.step().</p>
<ol class="arabic">
<li><p><code class="docutils literal notranslate"><span class="pre">STEP_EVERY_EPOCH</span></code>: Determined will call scheduler.step() after
every <code class="docutils literal notranslate"><span class="pre">frequency</span></code> training epoch(s). No arguments will be passed to step().</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">STEP_EVERY_BATCH</span></code>: Determined will call scheduler.step() after every
<code class="docutils literal notranslate"><span class="pre">frequency</span></code> training batch(es). No arguments will be passed to step().
This option does not take into account gradient aggregation;
<code class="docutils literal notranslate"><span class="pre">STEP_EVERY_OPTIMIZER_STEP</span></code> which is recommended.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">STEP_EVERY_OPTIMIZER_STEP</span></code>: Determined will call scheduler.step() in sync
with optimizer steps. With <code class="docutils literal notranslate"><span class="pre">optimizations.aggregation_frequency</span></code> unset, this
is equivalent to <code class="docutils literal notranslate"><span class="pre">STEP_EVERY_BATCH</span></code>; when it is set, it ensures the LR
scheduler is stepped every _effective_ batch.</p>
<p>If the option <code class="docutils literal notranslate"><span class="pre">frequency</span></code> is set to some value N, Determined will step the LR
scheduler every N optimizer steps.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">MANUAL_STEP</span></code>: Determined will not call scheduler.step() at all.
It is up to the user to decide when to call scheduler.step(),
and whether to pass any arguments.</p></li>
</ol>
</dd>
<dt>frequency:</dt><dd><p>Sets the frequency at which the batch and epoch step modes get triggered.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="determined-pytorch-reducer">
<h2><code class="docutils literal notranslate"><span class="pre">determined.pytorch.Reducer</span></code><a class="headerlink" href="#determined-pytorch-reducer" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="determined.pytorch.Reducer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">determined.pytorch.</span></span><span class="sig-name descname"><span class="pre">Reducer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#determined.pytorch.Reducer" title="Permalink to this definition">#</a></dt>
<dd><p>A <code class="docutils literal notranslate"><span class="pre">Reducer</span></code> defines a method for reducing (aggregating) evaluation
metrics. See <a class="reference internal" href="#determined.pytorch.PyTorchTrial.evaluation_reducer" title="determined.pytorch.PyTorchTrial.evaluation_reducer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluation_reducer()</span></code></a> for
details.</p>
<dl class="simple">
<dt>Attributes:</dt><dd><p>AVG
SUM
MAX
MIN</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="determined-pytorch-metricreducer">
<span id="pytorch-metric-reducer"></span><h2><code class="docutils literal notranslate"><span class="pre">determined.pytorch.MetricReducer</span></code><a class="headerlink" href="#determined-pytorch-metricreducer" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="determined.pytorch.MetricReducer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">determined.pytorch.</span></span><span class="sig-name descname"><span class="pre">MetricReducer</span></span><a class="headerlink" href="#determined.pytorch.MetricReducer" title="Permalink to this definition">#</a></dt>
<dd><p>Efficiently aggregating validation metrics during a multi-slot distributed trial is done in
three steps:</p>
<ol class="arabic simple">
<li><p>Gather all the values to be reduced during the reduction window (either a training or a
validation workload).  In a multi-slot trial, this is done on each slot in parallel.</p></li>
<li><p>Calculate the per-slot reduction.  This will return some intermediate value that each slot
will contribute to the final metric calculation.  It can be as simple as a list of all the
raw values from step 1, but reducing the intermediate value locally will distribute the
final metric calculation more efficiently and will reduce network communication costs.</p></li>
<li><p>Reduce the per-slot reduction values from Step 2 into a final metric.</p></li>
</ol>
<p>The MetricReducer API makes it possible for users to define a maximally efficient custom metric
by exposing these steps to users:</p>
<blockquote>
<div><ul class="simple">
<li><p>Step 1 is defined by the user; it is not part of the interface.  This flexibility
gives the user full control when gathering individual values for reduction.</p></li>
<li><p>Step 2 is the MetricReducer.per_slot_reduce() interface.</p></li>
<li><p>Step 3 is the MetricReducer.cross_slot_reduce() interface.</p></li>
<li><p>The MetricReducer.reset() interface allows for MetricReducer reuse across many train and
validation workloads.</p></li>
</ul>
</div></blockquote>
<p>Example implementation and usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyAvgMetricReducer</span><span class="p">(</span><span class="n">pytorch</span><span class="o">.</span><span class="n">MetricReducer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">counts</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># User-defined mechanism for collecting values throughout</span>
    <span class="c1"># training or validation. This update() mechanism demonstrates</span>
    <span class="c1"># a computationally- and memory-efficient way to store the values.</span>
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sum</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">counts</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">per_slot_reduce</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Because the chosen update() mechanism is so</span>
        <span class="c1"># efficient, this is basically a noop.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">counts</span>

    <span class="k">def</span> <span class="nf">cross_slot_reduce</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">per_slot_metrics</span><span class="p">):</span>
        <span class="c1"># per_slot_metrics is a list of (sum, counts) tuples</span>
        <span class="c1"># returned by the self.pre_slot_reduce() on each slot</span>
        <span class="n">sums</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">per_slot_metrics</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">sums</span><span class="p">)</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MyPyTorchTrial</span><span class="p">(</span><span class="n">pytorch</span><span class="o">.</span><span class="n">PyTorchTrial</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="c1"># Register your custom reducer.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">my_avg</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">wrap_reducer</span><span class="p">(</span>
            <span class="n">MyAvgMetricReducer</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;my_avg&quot;</span>
        <span class="p">)</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">train_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">epoch_idx</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="c1"># You decide how/when you call update().</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">my_avg</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">my_val</span><span class="p">)</span>

        <span class="c1"># The &quot;my_avg&quot; metric will be included in the final</span>
        <span class="c1"># metrics after the workload has completed; no need</span>
        <span class="c1"># to return it here.</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>
</pre></div>
</div>
<p>See also: <code class="xref py py-meth docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchExperimentalContext.wrap_reducer()</span></code>.</p>
<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.MetricReducer.reset">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.MetricReducer.reset" title="Permalink to this definition">#</a></dt>
<dd><p>Reset reducer state for another set of values.</p>
<p>This will be called before any train or validation workload begins.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.MetricReducer.per_slot_reduce">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">per_slot_reduce</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="headerlink" href="#determined.pytorch.MetricReducer.per_slot_reduce" title="Permalink to this definition">#</a></dt>
<dd><p>This will be called after all workers have finished (even when there is only one worker).</p>
<p>It should return some picklable value that is meaningful for cross_slot_reduce.</p>
<p>This will be called after any train or validation workload ends.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.MetricReducer.cross_slot_reduce">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">cross_slot_reduce</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">per_slot_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="headerlink" href="#determined.pytorch.MetricReducer.cross_slot_reduce" title="Permalink to this definition">#</a></dt>
<dd><p>This will be called after per_slot_reduce has finished (even when there is only one worker).</p>
<p>The per_slot_metrics will be a list containing the output of per_slot_reduce() from each
worker.</p>
<dl class="simple">
<dt>The return value should either be:</dt><dd><ul class="simple">
<li><p>A dict mapping string metric names to metric values, if the call to
context.wrap_reducer() omitted the <code class="docutils literal notranslate"><span class="pre">name</span></code> parameter, or</p></li>
<li><p>A non-dict metric value if the call to context.wrap_reducer() had name set to a string
(an error will be raised if a dict-type metric is returned but name was set).</p></li>
</ul>
</dd>
</dl>
<p>This will be called after per_slot_reduce.</p>
</dd></dl>

</dd></dl>

</section>
<section id="determined-pytorch-pytorchcallback">
<span id="pytorch-callbacks"></span><h2><code class="docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchCallback</span></code><a class="headerlink" href="#determined-pytorch-pytorchcallback" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">determined.pytorch.</span></span><span class="sig-name descname"><span class="pre">PyTorchCallback</span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback" title="Permalink to this definition">#</a></dt>
<dd><p>Abstract base class used to define a callback that should execute during
the lifetime of a PyTorchTrial or DeepSpeedTrial.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you are defining a stateful callback (e.g., it mutates a <code class="docutils literal notranslate"><span class="pre">self</span></code>
attribute over its lifetime), you must also override <a class="reference internal" href="#determined.pytorch.PyTorchCallback.state_dict" title="determined.pytorch.PyTorchCallback.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a> and
<a class="reference internal" href="#determined.pytorch.PyTorchCallback.load_state_dict" title="determined.pytorch.PyTorchCallback.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> to ensure this state can be serialized and deserialized
over checkpoints.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If distributed training is enabled, every GPU will execute a copy of this callback (except
for <a class="reference internal" href="#determined.pytorch.PyTorchCallback.on_checkpoint_write_end" title="determined.pytorch.PyTorchCallback.on_checkpoint_write_end"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_checkpoint_write_end()</span></code></a> during PyTorchTrial training, which is only called on
the chief). To configure a callback implementation to execute on a subset of GPUs, please
condition your implementation on <code class="docutils literal notranslate"><span class="pre">trial.context.distributed.get_rank()</span></code>.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.load_state_dict" title="Permalink to this definition">#</a></dt>
<dd><p>Load the state of this using the deserialized <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.on_checkpoint_end">
<span class="sig-name descname"><span class="pre">on_checkpoint_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.on_checkpoint_end" title="Permalink to this definition">#</a></dt>
<dd><p>Deprecated. Please use <a class="reference internal" href="#determined.pytorch.PyTorchCallback.on_checkpoint_write_end" title="determined.pytorch.PyTorchCallback.on_checkpoint_write_end"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_checkpoint_write_end()</span></code></a> instead.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This callback only executes on the chief GPU when doing distributed training with
PyTorchTrial.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.on_checkpoint_load_start">
<span class="sig-name descname"><span class="pre">on_checkpoint_load_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.on_checkpoint_load_start" title="Permalink to this definition">#</a></dt>
<dd><p>Run before state_dict is restored.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.on_checkpoint_save_start">
<span class="sig-name descname"><span class="pre">on_checkpoint_save_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.on_checkpoint_save_start" title="Permalink to this definition">#</a></dt>
<dd><p>Run before checkpoint is persisted.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.on_checkpoint_upload_end">
<span class="sig-name descname"><span class="pre">on_checkpoint_upload_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">uuid</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.on_checkpoint_upload_end" title="Permalink to this definition">#</a></dt>
<dd><p>Run after every checkpoint finishes uploading.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.on_checkpoint_write_end">
<span class="sig-name descname"><span class="pre">on_checkpoint_write_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.on_checkpoint_write_end" title="Permalink to this definition">#</a></dt>
<dd><p>Run after every checkpoint finishes writing to checkpoint_dir.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This callback only executes on the chief GPU when doing distributed training with
PyTorchTrial.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.on_training_epoch_end">
<span class="sig-name descname"><span class="pre">on_training_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.on_training_epoch_end" title="Permalink to this definition">#</a></dt>
<dd><p>Run on end of a training epoch</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.on_training_epoch_start">
<span class="sig-name descname"><span class="pre">on_training_epoch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.on_training_epoch_start" title="Permalink to this definition">#</a></dt>
<dd><p>Run on start of a new training epoch</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.on_training_start">
<span class="sig-name descname"><span class="pre">on_training_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.on_training_start" title="Permalink to this definition">#</a></dt>
<dd><p>Run after checkpoint loads and before training begins.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.on_training_workload_end">
<span class="sig-name descname"><span class="pre">on_training_workload_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">avg_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.on_training_workload_end" title="Permalink to this definition">#</a></dt>
<dd><p>Run on end of a training workload. Workloads can contain varying numbers of batches. In the
current implementation of PyTorchTrial and DeepSpeedTrial, the maximum number of batches in
a workload is equal to the <code class="docutils literal notranslate"><span class="pre">scheduling_unit</span></code> field defined in the experiment config.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.on_trial_shutdown">
<span class="sig-name descname"><span class="pre">on_trial_shutdown</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.on_trial_shutdown" title="Permalink to this definition">#</a></dt>
<dd><p>Runs just before shutting down training to get off of the cluster.  This does not imply that
the trial is complete; it may just be paused or preempted by a higher-priority task.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This callback runs each time a Trial shuts down gracefully to come off the cluster.
This callback does not mean that the Trial is done training.  Additionally, if the trial
is killed the container will be destroyed without this callback running.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.on_trial_startup">
<span class="sig-name descname"><span class="pre">on_trial_startup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">first_batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_uuid</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.on_trial_startup" title="Permalink to this definition">#</a></dt>
<dd><p>Runs before training, validation, or building dataloaders.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>first_batch_idx (int):  The first batch index to be trained.  If the trial has already</dt><dd><p>completed some amount of training in a previous allocation on the cluster, this will
be nonzero.</p>
</dd>
<dt>checkpoint_uuid (str or None):  The checkpoint from which weight, optimizer state, etc.</dt><dd><p>will be loaded.  When <code class="docutils literal notranslate"><span class="pre">first_batch_idx</span> <span class="pre">&gt;</span> <span class="pre">0</span></code> this will contain the uuid of the
most recent checkpoint saved by this trial.  Otherwise, it will contain the uuid of
the checkpoint from which this trial was configured to warm start from (via
<code class="docutils literal notranslate"><span class="pre">source_trial_id</span></code> or <code class="docutils literal notranslate"><span class="pre">source_checkpoint_uuid</span></code> in the searcher config), or None
if no warm start was configured.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.on_validation_end">
<span class="sig-name descname"><span class="pre">on_validation_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.on_validation_end" title="Permalink to this definition">#</a></dt>
<dd><p>Run after every validation ends.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.on_validation_epoch_end">
<span class="sig-name descname"><span class="pre">on_validation_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.on_validation_epoch_end" title="Permalink to this definition">#</a></dt>
<dd><p>Run after a new validation epoch has finished</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.on_validation_epoch_start">
<span class="sig-name descname"><span class="pre">on_validation_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.on_validation_epoch_start" title="Permalink to this definition">#</a></dt>
<dd><p>Run on start of a new validation epoch</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.on_validation_start">
<span class="sig-name descname"><span class="pre">on_validation_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.on_validation_start" title="Permalink to this definition">#</a></dt>
<dd><p>Run before every validation begins.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.PyTorchCallback.state_dict">
<span class="sig-name descname"><span class="pre">state_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#determined.pytorch.PyTorchCallback.state_dict" title="Permalink to this definition">#</a></dt>
<dd><p>Serialize the state of this callback to a dictionary. Return value must
be pickle-able.</p>
</dd></dl>

</dd></dl>

</section>
<section id="determined-pytorch-load-trial-from-checkpoint-path">
<h2><code class="docutils literal notranslate"><span class="pre">determined.pytorch.load_trial_from_checkpoint_path</span></code><a class="headerlink" href="#determined-pytorch-load-trial-from-checkpoint-path" title="Permalink to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="determined.pytorch.load_trial_from_checkpoint_path">
<span class="sig-prename descclassname"><span class="pre">determined.pytorch.</span></span><span class="sig-name descname"><span class="pre">load_trial_from_checkpoint_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trial_class</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Type</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#determined.pytorch.PyTorchTrial" title="determined.pytorch._pytorch_trial.PyTorchTrial"><span class="pre">PyTorchTrial</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trial_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch_load_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#determined.pytorch.PyTorchTrial" title="determined.pytorch._pytorch_trial.PyTorchTrial"><span class="pre">PyTorchTrial</span></a></span></span><a class="headerlink" href="#determined.pytorch.load_trial_from_checkpoint_path" title="Permalink to this definition">#</a></dt>
<dd><p>Loads a checkpoint written by a PyTorchTrial.</p>
<p>You should have already downloaded the checkpoint files, likely with
<a class="reference internal" href="../python-sdk.html#determined.experimental.client.Checkpoint.download" title="determined.experimental.client.Checkpoint.download"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Checkpoint.download()</span></code></a>.</p>
<p>The return value will be a restored instance of the subclass PyTorchTrial you used for training.</p>
<dl>
<dt>Arguments:</dt><dd><p>path (string): Top level directory to load the checkpoint from.
trial_class (optional): Provide your PyTorchTrial class to be loaded.  Only necessary if</p>
<blockquote>
<div><p>the automatic import logic is insufficient.</p>
</div></blockquote>
<dl class="simple">
<dt>trial_kwargs (optional): Additional keyword arguments to be passed to your PyTorchTrial</dt><dd><p>class, in addition to the context, which will always be the first positional parameter.</p>
</dd>
<dt>torch_load_kwargs (optional): Keyword arguments for <code class="docutils literal notranslate"><span class="pre">torch.load</span></code>. See documentation for</dt><dd><p><a class="reference external" href="https://pytorch.org/docs/stable/torch.html?highlight=torch%20load#torch.load">torch.load</a>.</p>
</dd>
</dl>
<p><a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs (deprecated): Use torch_load_kwargs instead.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="determined-pytorch-trainer">
<h2><code class="docutils literal notranslate"><span class="pre">determined.pytorch.Trainer</span></code><a class="headerlink" href="#determined-pytorch-trainer" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="determined.pytorch.Trainer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">determined.pytorch.</span></span><span class="sig-name descname"><span class="pre">Trainer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trial</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#determined.pytorch.PyTorchTrial" title="determined.pytorch._pytorch_trial.PyTorchTrial"><span class="pre">PyTorchTrial</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#determined.pytorch.PyTorchTrialContext" title="determined.pytorch._pytorch_context.PyTorchTrialContext"><span class="pre">PyTorchTrialContext</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#determined.pytorch.Trainer" title="Permalink to this definition">#</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">pytorch.Trainer</span></code> is an abstraction on top of a vanilla PyTorch training loop that handles
many training details under-the-hood, and exposes APIs for configuring training-related features
such as automatic checkpointing, validation, profiling, metrics reporting, etc.</p>
<p><code class="docutils literal notranslate"><span class="pre">Trainer</span></code> must be initialized and called from within a <code class="docutils literal notranslate"><span class="pre">pytorch.PyTorchTrialContext</span></code>.</p>
<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.Trainer.configure_profiler">
<span class="sig-name descname"><span class="pre">configure_profiler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sync_timings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enabled</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">begin_on_batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end_after_batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.Trainer.configure_profiler" title="Permalink to this definition">#</a></dt>
<dd><p>Configures the Determined profiler. This method should only be called before .fit(), and
only once within the scope of init(). If called multiple times, the last call’s
configuration will be used.</p>
<dl>
<dt>Arguments:</dt><dd><dl class="simple">
<dt>sync_timings: Specifies whether Determined should wait for all GPU kernel streams</dt><dd><p>before considering a timing as ended. Defaults to ‘true’. Applies only for
frameworks that collect timing metrics (currently just PyTorch).</p>
</dd>
</dl>
<p>enabled: Defines whether profiles should be collected or not. Defaults to false.
begin_on_batch: Specifies the batch on which profiling should begin.
end_after_batch: Specifies the batch after which profiling should end.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="determined.pytorch.Trainer.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_period:</span> <span class="pre">~determined.pytorch._pytorch_trial.TrainUnit</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_period:</span> <span class="pre">~determined.pytorch._pytorch_trial.TrainUnit</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length:</span> <span class="pre">~determined.pytorch._pytorch_trial.TrainUnit</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reporting_period:</span> <span class="pre">~determined.pytorch._pytorch_trial.TrainUnit</span> <span class="pre">=</span> <span class="pre">&lt;determined.pytorch._pytorch_trial.Batch</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_policy:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'best'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">latest_checkpoint:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step_zero_validation:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_mode:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#determined.pytorch.Trainer.fit" title="Permalink to this definition">#</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">fit()</span></code> trains a <code class="docutils literal notranslate"><span class="pre">PyTorchTrial</span></code> configured from the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> and handles
checkpointing and validation steps, and metrics reporting.</p>
<dl>
<dt>Arguments:</dt><dd><dl class="simple">
<dt>checkpoint_period: The number of steps to train for before checkpointing. This is</dt><dd><p>a <code class="docutils literal notranslate"><span class="pre">TrainUnit</span></code> type (<code class="docutils literal notranslate"><span class="pre">Batch</span></code> or <code class="docutils literal notranslate"><span class="pre">Epoch</span></code>) which can take an <code class="docutils literal notranslate"><span class="pre">int</span></code> or
instance of <code class="docutils literal notranslate"><span class="pre">collections.abc.Container</span></code> (list, tuple, etc.). For example,
<code class="docutils literal notranslate"><span class="pre">Batch(100)</span></code> would checkpoint every 100 batches, while <code class="docutils literal notranslate"><span class="pre">Batch([5,</span> <span class="pre">30,</span> <span class="pre">45])</span></code>
would checkpoint after every 5th, 30th, and 45th batch.</p>
</dd>
<dt>validation_period: The number of steps to train for before validating. This is a</dt><dd><p><code class="docutils literal notranslate"><span class="pre">TrainUnit</span></code> type (<code class="docutils literal notranslate"><span class="pre">Batch</span></code> or <code class="docutils literal notranslate"><span class="pre">Epoch</span></code>) which can take an <code class="docutils literal notranslate"><span class="pre">int</span></code> or instance
of <code class="docutils literal notranslate"><span class="pre">collections.abc.Container</span></code> (list, tuple, etc.). For example, <code class="docutils literal notranslate"><span class="pre">Batch(100)</span></code>
would validate every 100 batches, while <code class="docutils literal notranslate"><span class="pre">Batch([5,</span> <span class="pre">30,</span> <span class="pre">45])</span></code> would validate
after every 5th, 30th, and 45th batch.</p>
</dd>
<dt>max_length: The maximum number of steps to train for. This value is required and</dt><dd><p>only applicable in local training mode. For on-cluster training, this value will
be ignored; the searcher’s <code class="docutils literal notranslate"><span class="pre">max_length</span></code> must be configured from the experiment
configuration. This is a <code class="docutils literal notranslate"><span class="pre">TrainUnit</span></code> type (<code class="docutils literal notranslate"><span class="pre">Batch</span></code> or <code class="docutils literal notranslate"><span class="pre">Epoch</span></code>) which takes an
<code class="docutils literal notranslate"><span class="pre">int</span></code>. For example, <code class="docutils literal notranslate"><span class="pre">Epoch(1)</span></code> would train for a maximum length of one epoch.
reporting_period:</p>
</dd>
</dl>
<p>checkpoint_policy: Controls how Determined performs checkpoints after validation
operations, if at all. Should be set to one of the following values:</p>
<blockquote>
<div><p>best (default): A checkpoint will be taken after every validation operation
that performs better than all previous validations for this experiment.
Validation metrics are compared according to the metric and smaller_is_better
options in the searcher configuration. This option is only supported for
on-cluster training.
all: A checkpoint will be taken after every validation, no matter the
validation performance.
none: A checkpoint will never be taken due to a validation. However,
even with this policy selected, checkpoints are still expected to be taken
after the trial is finished training, due to cluster scheduling decisions,
before search method decisions, or due to min_checkpoint_period.</p>
</div></blockquote>
<dl class="simple">
<dt>latest_checkpoint: Configures the checkpoint used to start or continue training.</dt><dd><p>This value should be set to <code class="docutils literal notranslate"><span class="pre">det.get_cluster_info().latest_checkpoint</span></code> for
standard continue training functionality.</p>
</dd>
<dt>step_zero_validation: Configures whether or not to perform an initial validation</dt><dd><p>before training.</p>
</dd>
<dt>test_mode: Runs a minimal loop of training for testing and debugging purposes. Will</dt><dd><p>train and validate one batch. Defaults to false.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="determined-pytorch-init">
<h2><code class="docutils literal notranslate"><span class="pre">determined.pytorch.init()</span></code><a class="headerlink" href="#determined-pytorch-init" title="Permalink to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="determined.pytorch.init">
<span class="sig-prename descclassname"><span class="pre">determined.pytorch.</span></span><span class="sig-name descname"><span class="pre">init</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_conf</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="api-core-reference.html#determined.core.DistributedContext" title="determined.core._distributed.DistributedContext"><span class="pre">DistributedContext</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aggregation_frequency</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Iterator</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#determined.pytorch.PyTorchTrialContext" title="determined.pytorch._pytorch_context.PyTorchTrialContext"><span class="pre">PyTorchTrialContext</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#determined.pytorch.init" title="Permalink to this definition">#</a></dt>
<dd><p>Creates a PyTorchTrialContext for use with a PyTorchTrial. All trainer.* calls must be within
the scope of this context because there are resources started in __enter__ that must be
cleaned up in __exit__.</p>
<dl>
<dt>Arguments:</dt><dd><p>hparams: (Optional) instance of hyperparameters for the trial
exp_conf: (Optional) for local-training mode. If unset, calling</p>
<blockquote>
<div><p>context.get_experiment_config() will fail.</p>
</div></blockquote>
<p>distributed: (Optional) custom distributed training configuration
aggregation_frequency: number of batches before gradients are exchanged in distributed</p>
<blockquote>
<div><p>training. This value is configured here because it is used in context.wrap_optimizer.</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

</section>
</section>


                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> {your-title}
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-pytorchtrial"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchTrial</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.trial_context_class"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.trial_context_class</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.__init__"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.__init__()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.train_batch"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.train_batch()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.build_training_data_loader"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.build_training_data_loader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.build_validation_data_loader"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.build_validation_data_loader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.build_callbacks"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.build_callbacks()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.evaluate_batch"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.evaluate_batch()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.evaluation_reducer"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.evaluation_reducer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.evaluate_full_dataset"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.evaluate_full_dataset()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrial.get_batch_length"><code class="docutils literal notranslate"><span class="pre">PyTorchTrial.get_batch_length()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-pytorchtrialcontext"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchTrialContext</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.backward"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.configure_apex_amp"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.configure_apex_amp()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.current_train_batch"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.current_train_batch()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_data_config"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_data_config()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_experiment_id"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_experiment_id()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_global_batch_size"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_global_batch_size()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_hparam"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_hparam()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_per_slot_batch_size"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_per_slot_batch_size()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_stop_requested"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_stop_requested()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_tensorboard_path"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_tensorboard_path()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_tensorboard_writer"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_tensorboard_writer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.get_trial_id"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.get_trial_id()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.is_epoch_end"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.is_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.is_epoch_start"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.is_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.set_profiler"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.set_profiler()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.set_stop_requested"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.set_stop_requested()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.step_optimizer"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.step_optimizer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.to_device"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.to_device()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.wrap_lr_scheduler"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.wrap_lr_scheduler()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.wrap_model"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.wrap_model()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.wrap_optimizer"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.wrap_optimizer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchTrialContext.wrap_scaler"><code class="docutils literal notranslate"><span class="pre">PyTorchTrialContext.wrap_scaler()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-pytorchtrialcontext-distributed"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchTrialContext.distributed</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-pytorchexperimentalcontext"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchExperimentalContext</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchExperimentalContext"><code class="docutils literal notranslate"><span class="pre">PyTorchExperimentalContext</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchExperimentalContext.disable_auto_to_device"><code class="docutils literal notranslate"><span class="pre">PyTorchExperimentalContext.disable_auto_to_device()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchExperimentalContext.disable_dataset_reproducibility_checks"><code class="docutils literal notranslate"><span class="pre">PyTorchExperimentalContext.disable_dataset_reproducibility_checks()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchExperimentalContext.use_amp"><code class="docutils literal notranslate"><span class="pre">PyTorchExperimentalContext.use_amp()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-dataloader"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.DataLoader</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.DataLoader"><code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-lrscheduler"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.LRScheduler</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.LRScheduler"><code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.LRScheduler.StepMode"><code class="docutils literal notranslate"><span class="pre">LRScheduler.StepMode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.LRScheduler.__init__"><code class="docutils literal notranslate"><span class="pre">LRScheduler.__init__()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-reducer"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.Reducer</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.Reducer"><code class="docutils literal notranslate"><span class="pre">Reducer</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-metricreducer"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.MetricReducer</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.MetricReducer"><code class="docutils literal notranslate"><span class="pre">MetricReducer</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.MetricReducer.reset"><code class="docutils literal notranslate"><span class="pre">MetricReducer.reset()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.MetricReducer.per_slot_reduce"><code class="docutils literal notranslate"><span class="pre">MetricReducer.per_slot_reduce()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.MetricReducer.cross_slot_reduce"><code class="docutils literal notranslate"><span class="pre">MetricReducer.cross_slot_reduce()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-pytorchcallback"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.PyTorchCallback</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.load_state_dict"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.load_state_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_checkpoint_end"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_checkpoint_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_checkpoint_load_start"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_checkpoint_load_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_checkpoint_save_start"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_checkpoint_save_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_checkpoint_upload_end"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_checkpoint_upload_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_checkpoint_write_end"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_checkpoint_write_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_training_epoch_end"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_training_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_training_epoch_start"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_training_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_training_start"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_training_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_training_workload_end"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_training_workload_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_trial_shutdown"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_trial_shutdown()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_trial_startup"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_trial_startup()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_validation_end"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_validation_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_validation_epoch_end"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_validation_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_validation_epoch_start"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_validation_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.on_validation_start"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.on_validation_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.PyTorchCallback.state_dict"><code class="docutils literal notranslate"><span class="pre">PyTorchCallback.state_dict()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-load-trial-from-checkpoint-path"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.load_trial_from_checkpoint_path</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.load_trial_from_checkpoint_path"><code class="docutils literal notranslate"><span class="pre">load_trial_from_checkpoint_path()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-trainer"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.Trainer</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.Trainer"><code class="docutils literal notranslate"><span class="pre">Trainer</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.Trainer.configure_profiler"><code class="docutils literal notranslate"><span class="pre">Trainer.configure_profiler()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.Trainer.fit"><code class="docutils literal notranslate"><span class="pre">Trainer.fit()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determined-pytorch-init"><code class="docutils literal notranslate"><span class="pre">determined.pytorch.init()</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determined.pytorch.init"><code class="docutils literal notranslate"><span class="pre">init()</span></code></a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By lb
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023, lb.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>